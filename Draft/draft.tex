\documentclass[11pt,letterpaper]{article}

\makeatletter
\renewcommand\paragraph{\@startsection{paragraph}{4}{\z@}%
                                      {1ex \@plus1ex \@minus.2ex}%
                                      {-1em}%
                                      {\normalfont\normalsize\bfseries}}
\makeatother

%%%%%%%%%%%%%%%%%%%%%
%  P A C K A G E S  %
%%%%%%%%%%%%%%%%%%%%%

% Authors
\usepackage{authblk}

% Page margins
\usepackage[margin=1in]{geometry}

% Nicer math font
\usepackage{mathpazo}

% More fancy lists
\usepackage{enumerate}

% Microtype
\usepackage{microtype}

% TikZ
\usepackage{tikz}
%\usetikzlibrary{calc,shapes.geometric}
\usetikzlibrary{backgrounds,fit,decorations.pathreplacing,calc}

% Highlights
\usepackage{soul}

% Young Tableaux
\usepackage{ytableau}

% Figure
\usepackage{float}

% Hypertext package
\usepackage[colorlinks = true]{hyperref}
% Title and authors
%\hypersetup{
%  pdftitle = {},
%  pdfauthor = {}
%}
% Color definitions
\definecolor{darkred}  {rgb}{0.5,0,0}
\definecolor{darkblue} {rgb}{0,0,0.5}
\definecolor{darkgreen}{rgb}{0,0.5,0}
% Color links
\hypersetup{
  urlcolor   = blue,         % color of external links
  linkcolor  = darkblue,     % color of internal links
  citecolor  = darkgreen,    % color of links to bibliography
  filecolor  = darkred       % color of file links
}

% AMS
\usepackage{amsmath,amssymb,amsfonts,amsthm,amstext}

%% Restating theorems
%\usepackage{thm-restate}

% Powerful macros
\usepackage{etoolbox}

% Fixes for amsmath
\usepackage{mathtools}
\mathtoolsset{centercolon}
\makeatletter
\protected\def\tikz@nonactivecolon{\ifmmode\mathrel{\mathop\ordinarycolon}\else:\fi}
\makeatother

% Daw boxes
\usepackage{tcolorbox}

% Code
\usepackage{algorithm}
%\usepackage{algorithmic}
\usepackage{algpseudocode}

% Clever references
\usepackage{cleveref}%[nameinlink]
\crefname{lemma}{Lemma}{Lemmas}
\crefname{proposition}{Proposition}{Propositions}
\crefname{definition}{Definition}{Definitions}
\crefname{theorem}{Theorem}{Theorems}
\crefname{conjecture}{Conjecture}{Conjectures}
\crefname{corollary}{Corollary}{Corollaries}
\crefname{claim}{Claim}{Claims}
\crefname{section}{Section}{Sections}
\crefname{appendix}{Appendix}{Appendices}
\crefname{figure}{Fig.}{Figs.}
\crefname{table}{Table}{Tables}
% \crefname{algorithm}{Algorithm}{Algorithms}

% IEEE tools
\usepackage[retainorgcmds]{IEEEtrantools}

% table of contents
\usepackage{tocloft}

% Table with multi-row
\usepackage{multirow}

%%%%%%%%%%%%%%%%%%%%%%%%%
%  N E W C O M M A N D  %
%%%%%%%%%%%%%%%%%%%%%%%%%

% Standard quantum notation

\newcommand{\ket}[1]{|#1\rangle}
\newcommand{\bra}[1]{\langle#1|}
\newcommand{\braket}[2]{\langle#1|#2\rangle}
\newcommand{\ketbra}[2]{|#1\rangle\langle#2|}
\newcommand{\proj}[1]{|#1\rangle\langle#1|}

\newcommand{\x}{\otimes}
\newcommand{\xp}[1]{^{\otimes #1}}
\newcommand{\op}{\oplus}

\newcommand{\ct}{^{\dagger}}
\newcommand{\tp}{^\intercal}

% Linear algebra

%\newcommand{\1}{\mathbb{1}} % identity matrix
%\DeclareMathOperator{\Hom}{Hom}
\DeclareMathOperator{\End}{End}
%\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\Lin}{L} % all linear maps
\newcommand{\Mat}[1]{\mathrm{M}(#1)} % all matrices
%\newcommand{\Mat}[1]{\mathrm{M}_{#1}(\C)}

% Paired delimiters

\DeclarePairedDelimiter{\set}{\lbrace}{\rbrace}
\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
\DeclarePairedDelimiter{\of}{\lparen}{\rparen}
\DeclarePairedDelimiter{\sof}{\lbrack}{\rbrack}
\DeclarePairedDelimiter{\ip}{\langle}{\rangle}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}

% Operators

\renewcommand{\Re}{\operatorname{Re}}
\renewcommand{\Im}{\operatorname{Im}}
\DeclareMathOperator{\vc}{vec}
\DeclareMathOperator{\spn}{span}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\spec}{spec}
\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\hook}{hook}
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\supp}{supp}


% Sets

\newcommand{\C}{\mathbb{C}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\calH}{\mathcal{H}}
\newcommand{\calX}{\mathcal{X}}
\newcommand{\calY}{\mathcal{Y}}
\newcommand{\calA}{\mathcal{A}}
\newcommand{\calB}{\mathcal{B}}


% Identity operator
\newcommand{\1}{\mathbb{1}}

% Pauli Group
\newcommand{\Pg}{\mathcal{P}}
\newcommand{\J}{\mathcal{J}}

% Special notation

\newcommand{\CHSH}{CHSH^{(d)}}
\newcommand{\MS}{MS}
\newcommand{\SVT}{SVT}
\newcommand{\EPR}[1]{EPR^{(#1)}}
\newcommand{\paulix}{\sigma_x}
\newcommand{\pauliz}{\sigma_z}
\newcommand{\G}{G}
\newcommand{\LS}{LS}
\newcommand{\tA}{\tilde{A}}
\newcommand{\tB}{\tilde{B}}
\newcommand{\tW}{\tilde{W}}
\newcommand{\tx}{\tilde{x}}
\newcommand{\tpsi}{\tilde{\psi}}
\newcommand{\tri}{\Delta}
\newcommand{\lB}{\overline{B}}
\newcommand{\dr}[1]{d^{(#1)}}

% Probabilities
\newcommand{\pr}[2]{P(#1|#2)}
\newcommand{\pa}[2]{P_A(#1|#2)}
\newcommand{\pb}[2]{P_B(#1|#2)}

% Bell Ineqaulities
\newcommand{\I}{\mathcal{I}}



%%%%%%%%%%%%%%%%%%%%%%%%%
%  N E W T H E O R E M  %
%%%%%%%%%%%%%%%%%%%%%%%%%

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{claim}[theorem]{Claim}
\newtheorem*{conjecture*}{Conjecture}
\newtheorem*{problem}{Problem}
\newtheorem*{example}{Example}

\theoremstyle{definition}
\newtheorem*{remark}{Remark}



%%%%%%%%%%%%%%%%
%   Document   %
%%%%%%%%%%%%%%%%

\begin{document}

\title{Self-test prime dimensional EPR pairs with constant alphabet}

\author[1]{Honghao Fu}
\author[1,2]{Carl Miller}

\renewcommand\Affilfont{\itshape\small}

\def\carl#1{{\color{blue} #1 -Carl}}

\affil[1]{Department of Computer Science, Institute for Advanced Computer Studies, and Joint Center for Quantum \break Information and Computer Science, University of Maryland, College Park, MD 20742, USA}
\affil[2]{National Institute of Standards and Technology, 100 Bureau Dr., Gaithersbug, MD 20899, USA}
\maketitle

%========================================
\section{Introduction}
\label{sec:intro}
%========================================
Self-testing is a unique phenomenon of quantum mechanics. It has many applications in quantum
delegated computation \cite{ruv2013,cgsv2017} and device independent quantum cryptography
\cite{qkd2011,qkd2014,miller2016,fu2018,eat2018}.

The case of self-testing $2$-dimensional EPR pair is fully understood. One can robustly self-test
one copy of it by the CHSH inequality \cite{bamps2015}. and self-test many copies of the $2$-dimensional EPR
pair in parallel \cite{mckague2016, coladan2017parallel}. 
Self-testing general $d$-dimensional EPR pairs is a harder task.
Recently, a remarkable result by Coladangelo \textit{et al}.\cite{cgs2017} 
has shown that the maximally entangled state with arbitrary local dimension 
can be self-tested with constant question alphabet but answer alphabet growing with
the local dimension. 
Then Coladangelo and Stark \cite{coladan2017} further showed that by playing many instances
of the generalized Magic Square game and Magic Pentagram game, one can robustly self-test
$N$ copies of the maximally entangled state with local dimension $d$ for any $d, N \geq 2$.
Both the Magic Square game and the Magic Pentagram game have constant question alphabet
but answer alphabet of size $d$.

At a high level, general $d$-dimensional maximally entangled states are self-tested by
modifying the correlation and enlarging the size of the correlation.
A natural question to ask is whether maximally entangled state with large local dimension
can be self-tested with fixed-sized correlation. 
An equivalent question to ask is whether it is possible to self-test maximally
entangled state of some local dimension more efficiently, with constant correlation size. 
In this report, we give an affirmative answer to this question by proving the following theorem.
\begin{theorem}[Informal]
\label{thm:inf}
	There exists an infinite-sized set $D$ of odd prime numbers such that, for any $d \in D$, 
	the maximally entangled state of local dimension $d-1$ can be self-tested 
	with constant-sized question and answer alphabets.
\end{theorem}

The set $D$ is easily characterizable as it contains all the odd prime numbers with smallest
primitive root $2$, $3$ or $5$. It has been shown that there are infinitely many prime numbers
with smallest primitive root in the set $\{2,3,5\}$ \cite{murty1988}, so the set $D$ has infinitely many elements.
To prove \cref{thm:inf}, we give explicit self-testing proof of 
the maximally entangled state with local dimension $d-1$ where the primitive root of $d$ is $2$,$3$, or $5$, 
by explicitly giving the correlation that achieves self-testing. 
%Then the self-testing property of this correlation is 
%given in the following theorem.
%\begin{theorem}[Informal]
%\label{thm:pr_2}
%	All maximally entangled state with local dimension $d-1$, where $d$ is prime and has
%	primitive root $r \in \{2, 3, 5\}$, can be self-tested by a constant-sized correlation.
%\end{theorem}
Our correlation is denoted by $C(\dr{r})$ for prime $d$.
We use the superscript $(r)$ to denote the primitive root of $d$.
Note that although the size of $C(\dr{r})$ does not depend on $d$, the optimal correlation does.


In order to accomplish our goal, we introduce new techniques for self-testing.
First of all, we use a different variant of the weighted CHSH inequality to enforce the eigenvalue of
some unknown operator which is the product of two binary observables used in the weighted CHSH test.
\carl{I think this game might have been defined in APM12, and if so we need to cite it.}
The variant of the CHSH inequality that we use is not used in the self-testing literature before.
Secondly, we give a new way to decompose unitaries of arbitrary order into binary observables which maintains
certain commutation relations. Such decomposition is different from what Slofstra used in his work \cite{slofstra2017}.
\carl{Our work is heavily based on \cite{slofstra2017}, and we need to make that clearer.}
Intuitively, such decomposition can be seen as the inverse of the Jordan's lemma decomposition.
The third contribution is that we prove self-testing without using anti-commutation relations between Pauli operators, 
which is the core idea in all the previous self-testing results.
\carl{Are we sure about that ("all")?}
Instead, we find a new pair of operators that can generate the ring of matrices over complex numbers.


\textbf{Structure of the paper}.
We start with notations and background information in \cref{sec:prelim}.
Since the correlation we designed can win a special linear system game and satisfy
an extended weighted CHSH test, we introduce the linear system game
in \cref{sec:lsg} and the extended weighted CHSH test in \cref{sec:chsh}. 
Our main result is based on the combination of the two tests and presented in \cref{sec:main}. 

%========================================
\section{Preliminaries and notations}
\label{sec:prelim}
%========================================
We use $[n]$ to denote the set $\{0,1 \dots n-1\}$ and $[n]+1$ for the set $\{1,2 \dots n\}$.
%We denote the group commutator of $A$ and $B$, i.e. $ABA^{-1}B^{-1}$, by $[A,B]$.

We use some basic number theory in our work. A primitive root of a prime number
$d$ is an integer $r$ such that $r \pmod{d}$ has multiplicative order $d-1$. Equivalently,
$r$ is the generator of the multiplicative group of integers modulo $d$, $\Z_d^\times$ where
$\Z_d^\times = \{1,2\dots d-1\}$. When we say $d$ has primitive root $r$, we always mean
that $r$ is the smallest primitive root.  \carl{I'm not clear on what that last sentence means.  You just defined what a primitive root
is --- are you changing the definition now?}

The EPR pair of local dimension $d-1$ for some prime $d$ is denoted by
\begin{align}
\ket{\EPR{d-1}} = \frac{1}{\sqrt{d-1}} \sum_{i = 0}^{d-2} \ket{ii}.
\end{align}
The superscript $(d-1)$ stresses the local dimension and we follow this convention through this paper.

We self-test $\ket{\EPR{d-1}}$ by verify that Alice and Bob has operators
\begin{align}
	X = \sum_{k=1}^{d-1} \omega_d^k\ketbra{k}{k} && U = \sum_{k=1}^{d-1} \ketbra{k/r}{k},
\end{align}
where $\omega_d = e^{i2\pi/d}$ is the primitive $d$-th root of unity,
and $r$ is the primitive root of $d$.  \carl{That sentence is jumping ahead --- you should just stick to
definitions and notation for now.}
Through out this paper, any operation on the label of the eigenvector $\ket{k}$ is taken modulo $d$,
unless otherwise specified, for example, the $k/r$ above.

\textbf{The weighted CHSH inequality \cite{acin2012}.}
The first building-block of our result is a robust self-testing result based on the weighted CHSH inequality.
In the CHSH scenario, Alice and Bob has binary observables $A_x,B_y$ for $x,y = 1,2$ 
on Hilbert space $\calH_A$ and $\calH_B$ respectively.  \carl{It needs to be clearer who Alice and Bob
are --- perhaps you should put this subsection after the ``Nonlocal games'' subsection.}
A binary observable is a Hermitian matrix whose only eigenvalues are either $+1$ or $-1$.
The weighted CHSH inequality is given by 
\begin{align}
	\label{eq:chsh_op}
	\I_\alpha = \alpha\ip{A_1B_1}+\alpha\ip{A_1B_2} + \ip{A_2B_1} - \ip{A_2B_2}\leq 2\alpha,
\end{align}
where $\ip{A_xB_y}$ is the expectation value of of the observables.  \carl{Same here -- it's not clear enough
what this means until the reader has seen the ``Nonlocal games'' subsection.}
If Alice and Bob share product state $\ket{\phi} = \ket{\phi_A} \x \ket{\phi_B}$, they cannot violate
the weighted CHSH inequality.
However, If they share entangled state $\ket{\psi}$, the maximal violation is 
\begin{align}
\label{eq:chsh_max}
 \I_\alpha \leq 2\sqrt{1+\alpha^2}.
\end{align}
\begin{definition}[Ideal strategy for $\I_\alpha$]
	\label{def:ideal}
	Define $\mu = \arctan(1/\alpha)$.
	The ideal strategy for weighted CHSH with parameter $\alpha$ (i.e. achieving maximal violation in \cref{eq:chsh_max})
	consists of the joint state $\ket{\EPR{2}}$ and observables $A_1 = \pauliz$, $A_2 = \paulix$,
	$B_1 = \cos(\mu) \pauliz+ \sin(\mu) \paulix$ and $B_2 = \cos(\mu) \pauliz - \sin(\mu) \paulix$.
\end{definition}
\carl{Just to be complete, you should say what $\pauliz$ and $\paulix$ denote.}
An interesting observation of the weighted CHSH inequality is that its maximal violation can certify the shared states and the 
measurements up to isometry, 
which is a phenomenon referred as a self-test. We give formal statement of this self-testing result in \cref{sec:chsh}.

\textbf{Nonlocal games}. The two players of a nonlocal game are Alice and Bob. Each of them is requested
to give an answer for a randomly chosen question. We denote Alice's question set by $\calX$ and answer set by $\calA$. Similarly,
Bob's question set is denoted by $\calY$ and his answer set is denoted by $\calB$. The nonlocal game also
comes with two functions: $\pi: \calX \times \calY \rightarrow [0,1]$, which is the probability distribution over the questions,
and $V: \calA \times \calB \times \calX \times \calY \rightarrow \R$, which is the scoring function. Such games are nonlocal
because Alice and Bob cannot communicate after getting their questions but they may share some strategy before 
the start of the game. Note that in the literature, the typical scoring function of a nonlocal game maps the input-output
pair to $\{0,1\}$ which corresponds to losing and winning. Allowing the score to be any real number is the key ingredient 
to our new result. 

A quantum strategy of a game $G$ consists of projective measurements $\{\{A_x^a\}_a\}$ on Alice's side, 
$\{\{B_y^b\}_b\}_y$ on Bob's side, and a shared state $\ket{\psi}$, where a projective measurement $P$ satisfies
the condition that $P^2 = P = P\ct$.  \carl{You need to put explicit labels on the quantum systems used by
Alice and Bob.  (Later on you seem to use ``$A$'' and ``$B$'' to refer to those systems, but those are also
the same letters you use for Alice's and Bob's measurements.  That's confusing.)}
Then Alice and Bob's quantum strategy produces the correlation
\begin{align}
	\pr{ab}{xy} = \bra{\psi} A_x^a \x B_y^b \ket{\psi} \text{ for all } (a,b,x,y) \in \calA \times \calB \times \calX \times \calY,
\end{align}
where $(A_x^a)^2 = A_x^a = (A_x^a)^\dagger$ and $(B_y^b)^2 = B_y^b = (B_y^b)^\dagger$.
%Note that we may use the term behaviour and correlation interchangeably.
%The value of a strategy is given by
%\begin{align}
%	\omega(G,p)  = \sum_{a,b,x,y} \pi(x,y) \pr{ab}{xy} V(a,b,x,y).
%\end{align} 

The main contribution of our work is the construction of a correlation, which is the combination of 
the optimal correlation of a linear system game with constant-sized $\calX$,$\calY$,$\calA$ and $\calB$
and the optimal correlation to violate the weighted CHSH inequality.  \carl{The reader does not 
know yet what a linear system game is, so this comment is more confusing than helpful.}
The special property of the correlation is that it can self-test $\ket{\EPR{d-1}}$ for arbitrary $d$ with certain 
primitive root. 
In the next part, we introduce the definition of self-testing, the definition of linear system game, 
and its operator solution, 
following definitions given in Ref.~\cite{coladan2017, slofstra2017}.

\carl{Before the next definition, you should define what a ``correlation'' is.}
\begin{definition}[Self-testing]
	We say that a correlation self-tests a quantum state $\ket{\psi}$, if 
	the correlation is produced by a strategy with shared state $\ket{\psi}$, and 
	for any quantum strategy $S = ( \{\tA_x\}, \{\tB_y\}, \ket{\tpsi})$ that produces the same correlation,
	there exists local isometries $\Phi_A$ and $\Phi_B$ on Alice and Bob's side and a state $\ket{junk}$ 
	such that 	
	\begin{align*}
		\Phi_A \x \Phi_B \ket{\tpsi} =  \ket{\psi} \x \ket{junk}.
	\end{align*}
\end{definition}
\begin{definition}[Linear system game]
 Let $Hx = c$ be an $m \times n$ system of linear equations over $\Z_2$,
 where $H$ is an $m$-by-$n$ matrix with entries in $\Z_2 = \{0,1\}$ and 
 $c$ is a length-$n$ vector with entries in $\Z_2$. 
 The associated linear system game involves two
 players Alice and Bob, where Alice is given an equation number $i \in \calX = [m]+1$ and replies with $a \in \calA = \Z_2^{\times n}$,
 and Bob is given a variable number $j \in \calY = [n]+1$ and replies with an assignment $b \in \calB = \Z_2$. The winning condition is 
 that Alice's assignment to the variables should satisfy equation $i$ and Alice's $j$-th assignment $a(j)$ should match $b$.
 Formally, the winning condition is 
 \begin{align*}
 	a(j) &= b && \text{(Consistency)} \\
	\sum_{k= 1}^n H(i,k) a(k) &\equiv c(i) \pmod 2. &&\text{(Constraint satisfaction)},
 \end{align*}
 for all $(i,j) \in \calX \times \calY$. 
\end{definition}
The scoring function of linear system games always maps an input-output pair to $\{0,1\}$, 
so later when we say a quantum strategy wins a linear system game perfectly, we mean that 
$V(a,b,x,y) =0$ implies that $\pr{ab}{xy} = 0$.  \carl{Make this clear in the definition itself.}
We focus on quantum strategies for the linear system game presented in terms of binary observables.
\begin{definition}[Quantum strategy of a linear system game]
\label{def:q_strat}
A quantum strategy for the linear system game $(Hx = c)$ consists of 
\begin{enumerate}
	\item a pair of finite Hilbert spaces $\calH_A$ and $\calH_B$;
	\item a collection of binary observables $B_j$, $1 \leq j \leq n$, on $\calH_B$
	such that $B_j^2 = \1$ for every $1 \leq j \leq n$;
	\item a collection of binary observables $A_{ij}$, $1\leq i \leq m$, $1\leq j\leq n$ 
	on $\calH_A$ such that 
	\begin{enumerate}
		\item $A_{ij}^2 = \1$ for every $i,j$,
		\item $\Pi_j A_{ij}^{H(i,j)} = (-\1)^{c(i)}$ for every $i$, and
		\item $A_{il}A_{ik} = A_{ik}A_{il}$ for every $i$ and $H(i,l) = H(i,k) =1$;
	\end{enumerate} 
	and
	\item a quantum state $\ket{\psi} \in \calH_A \x \calH_B$.
\end{enumerate}
\end{definition}
Note that any quantum strategy presented in terms of binary observables can be 
converted to a quantum strategy presented in terms of projective measurement, and
vice versa.  \carl{This is not entirely true.  A ``quantum strategy presented in terms of projective measurement'' allows
Alice to give answers that violate the ``Constraint satisfaction'' condition, while a ``quantum strategy presented 
in terms of binary observables'' does not.}

It has been shown in Ref.~\cite{cleve2014} that the linear system game has a perfect strategy 
satisfying conditions in \cref{def:q_strat} if and only if the linear system has a finite-dimensional
operator solution in the following sense.  \carl{There's a missing reference here?}
\begin{definition}[Operator solution of a linear system]
\label{def:op_sol}
	An operator solution to a linear system $Hx =c$ over $\Z_2$ is a sequence of bounded Hermitian 
	operators $A_1, A_2, \dots A_n$ on a Hilbert space $\calH$ such that \carl{(What does the word ``bounded'' mean here?  It might
	be best to just state the definition for finite dimensions only.)}
	\begin{enumerate}
		\item $A_i^2 = \1$, i.e. $A_i$ is a binary observable, for all $1 \leq i \leq n$;
		\item If $x_l$ and $x_k$ appear in the same equation, then $A_l$ and $A_k$ commute;
		\item for all $1 \leq i \leq m$,
		\begin{align*}
			\Pi_{k=1}^n A_k^{H(i,k)} = (-1)^{c(i)}\1.
		\end{align*}
	\end{enumerate}
	A finite dimensional operator solution to a linear system $Hx = c$ over $\Z_2$ is an operator
	solution in which the Hilbert space is finite-dimensional.
\end{definition}
Perfect quantum strategies can be extracted from operator solutions and vice versa.
Another angle to look at the linear system game ($Hx = c$) is a finitely presented group
over $\Z_2$, which is called the solution group.
\begin{definition}[Solution group of a linear system game]
	\label{def:presentation}
	Let $Hx = c$ be an $m \times n$  linear system. The solution group of this system
	is the group
	\begin{align*}
		\Gamma(H,c) := \ip{
		x_1,\dots x_n, J: &J^2 = x_i^2 = e \text{ for all }1 \leq i \leq n, \\
				& \Pi_{j=1}^n x_j^{H(i,j)} = J^{c(i)} \text{ for all } 1 \leq i \leq m, \text{ and } \\
				& x_l x_k = x_k x_l \text{ if } H(i,k) = H(i,l) = 1 \text{ for some } i
				} .
	\end{align*}
\end{definition}
\carl{Don't you also need a commutativity condition for $J$ with $\{ x_i \}$?}
For concepts about group presentations, we refer to Sec.~$2$ of Ref.~\cite{slofstra2017}.
Combining \cref{def:op_sol} and \cref{def:presentation}, we know that an operator solution associated to $Hx =c$ is 
a finite-dimensional representation of $\Gamma(H,c)$ that maps $J$ to $-\1$.

\carl{Section 2 needs to be cleaned up in some places, but overall it is pretty good.}


%The main tool to understand linear system game is through its solution group over $\Z_d$.
%\begin{definition}[Solution group over $\Z_d$ \cite{coladan2017}]
%	For the linear system game associated with $Ax = b$ over $\Z_d$, its solution group $\Gamma(A,b,\Z_d)$ has 
%	one generator for each variable and one relation for each equation and relations enforcing that the variables in the same
%	equation commutes. The set of local commutativity relations is denoted by $R_c$ and defined by
%	\begin{align}
%		R_c := \set{ [x_i, x_j] |  A_{li} \neq 0 \neq A_{lj} \text{ for some } 1 \leq l \leq m}.
%	\end{align}
%	The set of constraint satisfaction relations is denoted by $R_{eq}$ and defined by.
%	\begin{align}
%		R_{eq} := \set{ \J^{-b(l)} \Pi_{i=1}^n  x_i^{A_{li}} | 1\leq l\leq m}
%	\end{align}
%	Then the solution group has presentation 
%	\begin{align}
%	\Gamma(A,b,\Z_d) := \ip{ \{x_i\}_{i=1}^n \cup \{\J\} : R_c \cup R_{eq} \cup \{ (x_i)^d, \J^d| 1 \leq i \leq n\}}. 
%	\end{align}
%\end{definition}
%Note that the relations $(x_i)^d$ and $\J^d$ ensure that we have solutions over $\Z_d$.
%Then the operator solution of $\Gamma(A,b,\Z_d)$ is given below.
%\begin{definition}[Operator solution]
%	An operator solution for the linear system game associated with $Ax =b$ over $\Z_d$ is a unitary representation
%	$\tau$ of $\Gamma(A,b,\Z_d)$ such that $\tau(\J) = \omega_d\1$. A conjugate operator solution is a unitary 
%	representation mapping $\J$ to $\overline{\omega_d}\1$.
%\end{definition}
%What has been established in Ref.\cite{cleve2017,coladan2017} is that we can construct a perfect strategy of a
%linear system game from its operator solution and vice versa.


%=======================================
\section{The linear system game}
\label{sec:lsg}
%=======================================
The goal of this section and the following two sections is to 
present a correlation that can self-test $\ket{\EPR{d-1}}$, 
where $d$ is prime and has primitive root $r \in \{2, 3, 5\}$,
and this correlation is denoted by $C(\dr{r})$.  \carl{Be more precise --- we are constructing
such a correlation for any $d$ which has $2, 3,$ or $5$ as a primitive root.}
The correlation we constructed is the optimal correlation for a game $\G(\dr{r})$, which is the combination of two tests.
In this section, we introduce the linear system game which tests a relation that should be satisfied by 
Alice and Bob's observables.

Slofstra's seminal work~\cite{slofstra2017} draws our attention to the relation $xyx^{-1} = y^2$.  \carl{That sentence
is too vague -- either make it a rigorous statement or leave it out.}
The main component of $\G(\dr{r})$ is a linear system game $\LS$, whose solution group is the embedding of the 
following group
\begin{align}
	\Pg_r= \langle u, x : uxu^{-1} = x^r \rangle, \text{ for } r \in \{2, 3,5\}.
\end{align}
In other words, relations in the presentation of the solution group of $\LS_r$ can be combined to derive the
relation $uxu^{-1} = x^r$.  
\begin{proposition}
	\label{prop:embed}
	The group $\Pg_r$ can be embedded into a linear system game $\LS_r$ over $\Z_2$,
	with constant numbers of variables and equations, where each equation involves $3$ variables.
	\carl{You need to make this a more precise statement.  (It sounds as if you're saying 
	that the numbers of variables and equations is independent of $r$, which I'm pretty sure is not true.)}
\end{proposition}
The process of embedding $\Pg_r$ into the solution group of $\LS$ follows the recipe given in the proofs of
Proposition~$4.8$, Lemma~$4.4$ and Proposition $4.2$ of Ref.~\cite{slofstra2017}.
The order of applying the results is the reverse of the order they are presented in Ref.~\cite{slofstra2017}.
We give details of this embedding process in \cref{sec:power2} for $r = 2$, \cref{sec:power3} for $r = 3$, 
and \cref{sec:power5} for $r = 5$. 

In the later parts of the paper, we use $n_r$ and $m_r$ to refer to the size of $\LS_r$.
The key feature of this embedding is that $\LS_r$ has constant-sized input-output alphabet
even if the generators $u$ and $x$ have arbitrary order. 
The product of the generators $x_1$ and $x_2$ from the solution group of $\LS_r$ plays the role 
of the generator $x$ in $\Pg_r$ and the product of the generators $x_3$ and $x_4$ plays the role
of the generator $u$ in $\Pg_r$.   \carl{It's reasonable to make a couple informal statements
at the beginning of a section, but you shouldn't be making
such vague statements at this point.  Everything said at this point in the section should have a precise mathematical
meaning.}
As shown in \cref{sec:power2}, \cref{sec:power3} and \cref{sec:power5}, 
$x_1$,$x_2$,$x_3$ and $x_4$ satisfy the relation
\begin{align}
	(x_3x_4)(x_1x_2)(x_4x_3) = (x_1x_2)^r.
\end{align}
Hence the perfect quantum strategy $(\ket{\psi}, \{A_i, B_i\}_{i=1}^n)$ should satisfy 
the key relation that 
\begin{align}
	(A_3A_4) (A_1A_2)(A_4A_3) \ket{\psi} = (A_1A_2)^r\ket{\psi}, 
\end{align}
and similarly on Bob's side.
If we denote the unitary corresponding to the generator $u$ of $\Pg_r$ by $U$ and the unitary
corresponding to $x$ by $X$, then $U = A_3A_4$, $X= A_1A_2$, and 
$UXU\ct \ket{\psi} = X^r \ket{\psi}$.
The special property of $U$ and $X$ is summarized in the following lemma. 

\begin{lemma}
	\label{lm:ux_independ}
	Suppose there exist unitaries $U$ and $X$ which have the following forms
	\begin{align}
		X = \sum_{i=1}^{d-1} \omega_d^i \ketbra{i}{i} && U = \sum_{i=1}^{d-1}\ketbra{i/r}{i},
	\end{align}
	where $d$ is an odd prime number with primitive root $r$,
	then the set $\{U^k X^l\}$ for $k=0,1\dots d-2$ and $l = 1,2\dots d-1$ forms a basis of 
	the ring of $(d-1)\times (d-1)$ matrices over $\C$.
\end{lemma}
Note that the unitaries $U$ and $X$ defined in the lemma above satisfy the condition $UXU^\dagger = X^r$.
In the self-test proof, this lemma will be a critical step.
\begin{proof}
We are going to show the $(d-1)^2$ matrices from the set $\{U^k X^l\}_{k \in[d-1], l \in [d-1]+1}$ are linearly independent.
Suppose there exists a set of complex numbers $\{ x_{k,l} \}_{k \in[d-1], l \in [d-1]+1}$
such that 
\begin{align}
	M = \sum_{k=0}^{d-2} \sum_{l=1}^{d-1} x_{k,l} U^k X^l = 0. 
\end{align}
We further assume that there exists a set of integers $\{ k_i \}_{i=1}^{d-1}$ such that $r^{k_i} \equiv i \pmod{d}$.
\carl{We shouldn't be making extra assumptions that are not stated in the lemma.  This looks to me more like
a definition of the variables $\{ k_i \}$, rather than an assumption.}
The fact that $r$ is a primitive root of $d$ guarantees that $k_i$'s are distinct.
Then we can group $\{x_{k,l}\}$ into vectors: $\ket{x_{k_1}}, \ket{x_{k_2}} \dots \ket{x_{k_{d-1}}}$,
where $\ket{x_{k_i}}= (x_{k_i, 1}, x_{k_i, 2} \dots x_{k_i, d-1})^\intercal$.
Our goal is equivalent to proving that $\ket{x_{k_i}} = 0$ for all $i$.

We start with proving that $\ket{x_{k_1}} = 0$.
Proving $\ket{x_{k_i}} = 0$ for other $i$ follows a similar argument, so we briefly
discuss about it in the end.
The entry $\bra{1}M\ket{1}$ can be expressed as  
\begin{align}
	\bra{1}M\ket{1} = \sum_{k=0}^{d-2}\sum_{l = 1}^{d-1}\sum_{i=1}^{d-1} x_{k, l}\omega_d^{il}\braket{1}{i/r^k}\braket{i}{1}.
\end{align}
For the term $\braket{1}{i/r^k}\braket{i}{1} \neq 0$ we must have $i = 1$ and $r^k \equiv 1 \pmod{d}$, or equivalently,
$k = k_1$. We can conclude that 
\begin{align}
	\bra{1}M\ket{1} = \sum_{l = 1}^{d-1} x_{k_1,l}\omega_d^l = 0. 
\end{align}
Similarly we can determine that for all $j = 1,2\dots d-1$,
\begin{align}
	\bra{j}M\ket{j} 
	=  \sum_{k=0}^{d-2}\sum_{l = 1}^{d-1}\sum_{i=1}^{d-1} x_{k, l}\omega_d^{il}\braket{j}{i/r^k}\braket{i}{j} 
	= \sum_{l = 1}^{d-1}x_{k_1,l}\omega_d^{jl} = 0.
\end{align}
Hence we get $d-1$ equations with $d-1$ variables, and the linear system is
\begin{align}
	W \ket{x_{k_1}} = 0,
\end{align}
where $W(m,n) = \omega_d^{mn}$. Then we define
\begin{align}
	\tW = 
	\begin{pmatrix}
	1 & 1 \\
	1 & W
	\end{pmatrix}.
\end{align}
First observe that $\tW$ is a Vandermonde matrix, hence it is non-singular.
Next, we define $\ket{\tx_{k_1}} = (0, x_{k_1,1}, \dots x_{k_1,d-1})^\intercal$ 
and prove that it satisfies the condition
that 
\begin{align}
	\tW \ket{\tx_{k_1}} = 0,
\end{align}
which involves $d$ equations. The last $d-1$ equations are given by the assumption and $M$.
We only need to prove that $\sum_{l=1}^d x_{k_1, l} = 0$, which is required by the first row of $\tW$.
It can be proved by summing the known $d-1$ equations as follows
\begin{align}
	0=\sum_{j = 1}^{d-1} \bra{j}M\ket{j}  
	=  \sum_{j=1}^{d-1}\sum_{l = 1}^{d-1}x_{k_1,l}\omega_d^{jl}
	=\sum_{l = 1}^{d-1}x_{k_1,l} (\sum_{j=1}^{d-1} \omega_d^{jl})
	= \sum_{l = 1}^{d-1}- x_{k_1,l}
\end{align}
where we have used the fact that $\sum_{j=1}^{d-1} \omega_d^{jl} =-1$ for all $l = 1,2\dots d-1$.
Since $\tW$ is non-singluar, we know $\ket{\tx_{k_1}} = 0$ which implies that $\ket{x_{k_1}} = 0$.

For $\ket{x_{k_a}}$, we look at entries $\{\bra{j}M\ket{aj}\}_{j=1}^{d-1}$ for $a = 2 \dots d-1$ and get equations
of the form
\begin{align}
	0 = \bra{j}M\ket{aj} = \sum_{l=1}^{d-1} x_{k_a, l} \omega_d^{ajl} 
\end{align}
The corresponding coefficient matrix has value $\omega_d^{amn}$ at coordinate $(m,n)$,
so it is also a submatrix of a Vandermonde matrix. Similar argument gives us that $\ket{x_{k_a}} = 0$.

To summarize, we have proven that $x_{k,l} = 0$ for all $k$ and $l$, which implies that the elements of the set
$\{ U^k X^l \}$ are linearly independent and forms a basis for the ring of all the $(d-1)\times(d-1)$ matrices over $\C$.
\end{proof}
Note that the primitive root $r$ in the lemma is not restricted to the set $\{2,3,5\}$, so this lemma works for any 
odd prime number $d$.

%\begin{align}
%	\Pg_d = \langle x, z, \J : x^d = z^d = \J^d = e, zxz^{-1}x^{-1} = \J, x\J x^{-1}\J^{-1}= z\J z^{-1}\J^{-1} = e\rangle. 
%\end{align}
%The goal of our modification is to construct $\Pg_{\LS}$ which has implicit $d$-dependence.
%We first introduce new generators $u_x$ and $u_z$ to $\Pg_d$, and replace the relation $x^d = z^d = e$ 
%by the following relations
%\begin{align}
%\label{eq:sim}
%	u_x x u_x = x^2, \quad
%	u_z z u_z = z^2.
%\end{align}
%Consequently, the $d$ dependency of $\G$ comes from constraints imposed by other components of the game,
%\footnote{Figuring out what $a$ is will take us one step closer to resolving Artin's Conjecture\cite{murty1988}.}
%but the alphabet sizes are determined by $\Pg_{\LS}$ and we will see why they are constant.
%Next, we drop the relation $\J^d = e$ and make the value of $\J$ determined by $x$ and $z$ in the 
%relation $xzx^{-1}z^{-1} = \J$.
%
%It can be easily checked that $\paulix{d}$ and $\pauliz{d}$ can be extended to a representation of $\Pg_{\LS}$ 
%where  $\paulix{d}$ and $\pauliz{d}$ are defined by
%\begin{align}
%	\paulix{d} = \sum_{i=0}^{d-1} \ketbra{i+1 \pmod{d} }{i}  \quad \quad
%	\pauliz{d} = \sum_{i=0}^{d-1} \omega_d^i \ketbra{i}{i}.
%\end{align}
%Moreover, if $U_x\paulix{d}U_x^\dagger = (\paulix{d})^2$ and $U_z\pauliz{d}U_z^\dagger = (\pauliz{d})^2$,
%we can verify that 
%\begin{align}
%	U_xU_z = \1 = U_zU_x,
%\end{align}
%Hence, we do not need both $u_x$ and $u_z$ as generators and we just need one of them.
%In the end, we define $\Pg_{\LS}$ by
%\begin{equation}
%\begin{aligned}
%	\Pg_{\LS} =  \langle x, z, u, \J :  &zxz^{-1}x^{-1} = \J, [x,\J]=[z,\J]=[u,\J] = e, \\
%	&uxu^{-1} = x^2, u^{-1}zu = z^2 \rangle. 
%\end{aligned}
%\end{equation}
%This group will be embedded in a solution group, $\Gamma(\LS)$, following Slofstra's embedding techniques.
%The corresponding game $\LS$ has $n = 2351$ variables and $m= 1916$ equations.

%======================================
\section{The extended weighted CHSH test}
\label{sec:chsh}
%======================================
Before introducing the extended weighted CHSH test, we fill in some background about the weighted CHSH inequality,
namely, that its maximal violation can self-test $2$-dimensional EPR pair and rotated Pauli operators.
The robust self-testing result is summarized in the following theorem.
\begin{theorem}
\label{thm:selftest}
	Suppose the quantum strategy $(\ket{\tpsi}, \{\tilde{A}_x\}_{x \in [2]+1}, \{\tilde{B}_y\}_{y \in [2]+1})$ achieves the violation
	at least $2\sqrt{1+\alpha^2} - \epsilon$
	for some $\epsilon$, then
	there exists a local isometry $\Phi = \Phi_A \x \Phi_B$ and an auxiliary state $\ket{aux}$  such that
	\begin{align}
		\| \Phi( \tilde{A}_x \x \tilde{B}_y \ket{\psi}) -\ket{junk} \x (A_x \x B_y) \ket{\EPR{2}}  \| = O((\alpha+\frac{1}{\alpha}) \sqrt{\epsilon})
	\end{align}
	for $x,y \in \{0, 1, 2\}$ where the subscript $0$ refers to the identity operator and $A_x, B_y$ are 
	defined in \cref{def:ideal}.
\end{theorem}
Our approach is very similar to the one used in Ref.~\cite{bamps2015} so we 
defer the proof of \cref{thm:selftest} till \cref{sec:selftest}.
After proving the robust self-testing result, we take one step further and observe an interesting property of the 
product of Bob's observables.
\begin{proposition}
\label{prop:2d-subspace}
	Let $\mu = \arctan(1/\alpha)$ for some $\alpha$.
	Suppose a quantum strategy $(\ket{\tpsi}, \{\tilde{A}_x\}_{x \in [2]+1}, \{\tilde{B}_y\}_{y \in [2]+1})$ achieves the maximal 
	violation of  $\I_{\alpha}$, then $\tilde{B}_0\tilde{B}_1$ has eigenvalues $e^{i2\mu}$ and $e^{-i2\mu}$.
\end{proposition}
\begin{proof}[Proof of \cref{prop:2d-subspace}]
	Following the argument in the proof of \cref{thm:selftest} in \cref{sec:selftest}, we know
	$\tilde{Z}_B = (\tB_1+\tB_2)/2\cos(\mu)$ is a binary observable. \textcolor{red}{HF: Do we need to justify it?}
	Expanding $\tilde{Z}_B^2$ we get
	\begin{align*}
		\1 = \tilde{Z}_B^2 = \frac{\tB_1^2 + \tB_1\tB_2 + \tB_2\tB_1 + \tB_2^2}{4\cos(\mu)^2}
		=\frac{2\1 + \tB_1\tB_2 + \tB_2\tB_1}{4\cos^2(\mu)},
	\end{align*}
	so we can derive that 
	\begin{align}
	\label{eq:eig_v}
		\tB_1\tB_2 + \tB_2\tB_1 = 2(2\cos^2(\mu)-1)\1 = 2 \cos(2\mu) \1.
	\end{align}
	Observe that $(\tB_1\tB_2)^\dagger = \tB_2\tB_1$ and 
	$(\tB_2\tB_1)(\tB_1\tB_2) = \tB_2\1\tB_2 = \1$, then we can conclude that $\tB_1\tB_2$ is unitary.
	Suppose the eigen-decomposition of $\tB_1\tB_2$ is
	\begin{align}
		\tB_1\tB_2 = \sum_{i=1}^m \lambda_i \ketbra{i}{i}
	\end{align}
	for some orthonormal set of eigenvectors $\{ \ket{i} \}_{i=1}^m$
	and $\norm{\lambda_i} =1$ for $1 \leq i \leq m$.
	Eq.~\ref{eq:eig_v} tells us that for each $1 \leq i \leq m$
	\begin{align*}
		\lambda_i + \lambda_i^{-1} = 2\Re(\lambda_i) = 2 \cos(2\mu),
	\end{align*}
	so we can conclude that $\lambda_i = e^{i 2\mu}$ or $e^{-i 2\mu}$.
	
%	\begin{align}
%		\ket{\tpsi} = \sum_{i=1}^m \frac{c_i}{\sqrt{2}} (\ket{u_{i,0}}\ket{u_{i,0}}+\ket{u_{i,1}}\ket{u_{i,1}}),
%	\end{align}
%	where $c_i$'s are coefficients such that $\sum_{i=1}^m \norm{c_i}^2 = 1$.
%	The special property of the subspace $V_i = \spn( \ket{u_{i,0}}, \ket{u_{i,1}})$ is that 
%	\begin{align*}
%	(\tilde{B}_0 + \tilde{B}_1) \ket{u_{i,0}} = 2\cos(-\pi/2d) \ket{u_{i,0}} 
%	&&(\tilde{B}_0 + \tilde{B}_1) \ket{u_{i,1}} = -2\cos(-\pi/2d) \ket{u_{i,1}}\\
%	(\tilde{B}_0 - \tilde{B}_1) \ket{u_{i,0}} = 2\sin(-\pi/2d) \ket{u_{i,1}} 
%	&&(\tilde{B}_0 - \tilde{B}_1) \ket{u_{i,1}} = 2\sin(-\pi/2d) \ket{u_{i,0}}.
%	\end{align*}
%	It is straightforward to calculate that 
%	\begin{align*}
%		\tilde{B}_0\tilde{B}_1 \ket{u_{i,0}} = \cos(\pi/d) \ket{u_{i,0}} -\sin(\pi/d) \ket{u_{i,1}},&&
%		\tilde{B}_0\tilde{B}_1\ket{u_{i,1}} = \sin(\pi/d)\ket{u_{i,0}} + \cos(\pi/d) \ket{u_{i,1}},
%	\end{align*}
%	so we can conclude that 
%	\begin{align*}
%		&\tilde{B}_0\tilde{B}_1(\ket{u_{i,0}} + i\ket{u_{i,1}}) = e^{i \frac{\pi}{d}} (\ket{u_{i,0}} + i\ket{u_{i,1}}), &&
%		&\tilde{B}_0\tilde{B}_1(\ket{u_{i,0}} - i\ket{u_{i,1}}) = e^{-i \frac{\pi}{d}} (\ket{u_{i,0}} - i\ket{u_{i,1}}).
%	\end{align*}
%	Hence each $V_i$ is spanned by $\omega_d$-eigenvector and $\omega_d^{-1}$-eigenvector of $\tB_0\tB_1$.
\end{proof}

The extended weighted CHSH test is added to make sure that the operator $X$ extracted from 
Alice and Bob's operator solution of $\LS$ has eigenvalues $\omega_d$ and $\omega_d^{d-1}$.
In \cref{sec:main}, we will reason why showing these two eigenvalues is enough to guarantee that 
$X$ has the eigen-structure required by \cref{lm:ux_independ}.
We denote this game that enforces the eigenvalues of the observable $X$ by
$\CHSH_X$, where the superscript $d$ emphasizes that the scoring rules of this game depend on $d$.
We remark that for the extended weighted CHSH test, the primitive root of $d$ is irrelevant,
so we drop the superscript $(r)$.


In this test, Alice and Bob each gets a question $x$,$y \in \{ 1, 2, \ast\}$ and 
they answer with $a,b \in \{0,1,\diamond,\perp\}$. 
The correlation $C(\dr{2})$ is the optimal correlation of this test.
Before presenting the optimal correlation, we give intuitions about how the players should behave.
\begin{itemize}
	\item \textbf{Case 1:} when $x = y = \ast$, Alice and Bob should answer with $a, b \in \{\diamond, \perp\}$ and 
	their answer should agree;
	\item \textbf{Case 2a:} when $x,y \in \{1,2\}$ and if they answer with $a,b \in \{0,1\}$, then
	their answers are scored according to $I_{\cot(-\pi/d)}$;
	\item \textbf{Case 2b:} when $x,y \in \{1,2\}$ and if Alice answers with $\perp$, then all Bob's answers are irrelevant;
	\item \textbf{Case 3:} when $x \in {1,2}, y = \ast$, if Bob answers $\diamond$, 
	Alice should answer with $\{0.1\}$ but not $\perp$,
	if Bob answers $\perp$, Alice should answer $\perp$ too.
\end{itemize}
\textbf{The ideal strategy and ideal correlation}. Alice and Bob share the state $\ket{\psi} =\frac{1}{\sqrt{d-1}} \sum_{i=1}^{d-1} \ket{u_i}\ket{u_i}$.
We define two subspaces $V = \spn\{\ket{u_1}, \ket{u_{d-1}}\}$ and $V^\perp = \C^d \setminus\spn\{\ket{u_1}, \ket{u_{d-1}}\}$ and
define $\Pi_V$ and $\Pi_{V}^\perp$ to be the corresponding projectors. Note that $V$ is the subspace on which they should
maximize $\langle I_{\cot(-\pi/d)} \rangle$.

For completeness, we show the ideal correlation in the following three charts and then give the projectors. 
Note that we don't explicitly calculate the conditional probabilities of the form $\pr{\perp 0}{xy}$ for all possible $x$,$y$ 
because they are irrelevant.
\begin{table}[H]
\begin{center}
\begin{tabular}{|c|c||c|c|}
\hline
\multicolumn{2}{|c|}{} &
\multicolumn{2}{|c|}{$x=\ast$}\\
\cline{3-4}
\multicolumn{2}{|c|}{} &$a = \diamond$ & $a = \perp$ \\
\hline
\hline
\multirow{2}{*}{$y = \ast$} & $b=\diamond$ & 2/(d-1) & 0 \\
\cline{2-4}
&$b=\perp$ & 0 & (d-3)/(d-1) \\
\hline
\end{tabular}
\caption{Alice and Bob's behaviour when $x=y=\ast$.}
\end{center}
\end{table}

\begin{table}[H]
\begin{center}
\begin{tabular}{|c|c||c|c|c|c|c|c|}
\hline
\multicolumn{2}{|c|}{} &
\multicolumn{3}{|c|}{$x=1$}&
\multicolumn{3}{|c|}{$x=2$} \\
\cline{3-8}
\multicolumn{2}{|c|}{} &
$a = 0$ & $a=1$ & $a=\perp$ &
$a = 0$ & $a=1$ & $a=\perp$\\
\hline
\hline
\multirow{2}{*}{$y = 1$} & $b=0$ & $\frac{\cos^2(\pi/2d)}{d-1}$ & $\frac{\sin^2(\pi/2d)}{d-1}$ & \small $\pr{\perp0}{00}$ 
& $\frac{1+\sin(\pi/d)}{2(d-1)}$ & $\frac{1-\sin(\pi/d)}{2(d-1)}$ & \small  $\pr{\perp0}{10}$ \\
\cline{2-8}
&$b=1$ & $\frac{\sin^2(\pi/2d)}{d-1}$ & $\frac{\cos^2(\pi/2d)}{d-1}$ & $\frac{d-3}{d-1}-\pr{\perp0}{00}$ 
&  $\frac{1-\sin(\pi/d)}{2(d-1)}$ & $\frac{1+\sin(\pi/d)}{2(d-1)}$ & \small $\frac{d-3}{d-1} - \pr{\perp0}{10}$  \\
\hline
\multirow{2}{*}{$y = 2$} & $b=0$ & $\frac{\cos^2(\pi/2d)}{d-1}$ & $\frac{\sin^2(\pi/2d)}{d-1}$ & \small $\pr{\perp0}{01}$ & 
$ \frac{1-\sin(\pi/d)}{2(d-1)}$ & $ \frac{1+\sin(\pi/d)}{2(d-1)}$ & \small $\pr{\perp 0}{11}$  \\
\cline{2-8}
&$b=1$ & $\frac{\sin^2(\pi/2d)}{d-1}$ & $\frac{\cos^2(\pi/2d)}{d-1}$ & \small $\frac{d-3}{d-1}-\pr{\perp0}{01}$ &  
$ \frac{1+\sin(\pi/d)}{2(d-1)}$ & $ \frac{1-\sin(\pi/d)}{2(d-1)}$ & \small $\frac{d-3}{d-1}- \pr{\perp 0}{11}$ \\
\hline
\end{tabular}
\end{center}
\caption{Alice and Bob's behaviour when $x,y \in [2]$.}
\end{table}

\begin{table}[H]
\begin{center}
\begin{tabular}{|c|c||c|c|c|c|c|c|}
\hline
\multicolumn{2}{|c|}{} &
\multicolumn{3}{|c|}{$x=1$}&
\multicolumn{3}{|c|}{$x=2$} \\
\cline{3-8}
\multicolumn{2}{|c|}{} &
$a = 0$ & $a=1$ & $a=\perp$ &
$a = 0$ & $a=1$ & $a=\perp$\\
\hline
\hline
\multirow{2}{*}{$y = \ast$} & $b=\diamond$ & $1/(d-1)$ & $1/(d-1)$ & 0 
& $1/(d-1)$ & $1/(d-1)$ & 0 \\
\cline{2-8}
&$b=\perp$ & 0 & 0 & $\frac{d-3}{d-1}$ 
&  0 & 0 & \small $\frac{d-3}{d-1} $  \\
\hline
\end{tabular}
\end{center}
\caption{Alice and Bob's behaviour when $x\in [2]$ and $y = \ast$.}
\end{table}
Alice's projectors are 
\begin{align*}
	&A_\ast^\diamond = \Pi_V, A_\ast^\perp = \Pi_V^\perp \\
	&A_1^0 = \ketbra{u_1}{u_1}, A_1^1 = \ketbra{u_{d-1}}{u_{d-1}}, A_1^\perp = \Pi_V^\perp\\
	&A_2^0 = \frac{1}{2}(\ket{u_1}+\ket{u_{d-1}})(\bra{u_1}+\bra{u_{d-1}}), 
	A_2^1 = \frac{1}{2}(\ket{u_1}-\ket{u_{d-1}})(\bra{u_1}-\bra{u_{d-1}}),A_2^\perp = \Pi_V^\perp.
\end{align*}
Bob's projectors are 
\begin{align*}
	&B_\ast^\diamond = \Pi_V, B_\ast^\perp = \Pi_V^\perp \\
	&B_1^0|_V = \left( \cos(\frac{\pi}{2d})\ket{u_1} + \sin(\frac{\pi}{2d})\ket{u_{d-1}}\right)
	\left( \cos(\frac{\pi}{2d})\bra{u_1} + \sin(\frac{\pi}{2d})\bra{u_{d-1}}\right)\\
	&B_1^1|_V = \left( \sin(\frac{\pi}{2d})\ket{u_1} - \cos(\frac{\pi}{2d})\ket{u_{d-1}}\right)
	\left( \sin(\frac{\pi}{2d})\bra{u_1} - \cos(\frac{\pi}{2d})\bra{u_{d-1}}\right)\\
	&B_2^0|_V = \left( \cos(\frac{\pi}{2d})\ket{u_1} - \sin(\frac{\pi}{2d})\ket{u_{d-1}}\right)
	\left( \cos(\frac{\pi}{2d})\bra{u_1} - \sin(\frac{\pi}{2d})\bra{u_{d-1}}\right)\\
	&B_2^1|_V = \left( \sin(\frac{\pi}{2d})\ket{u_1} + \cos(\frac{\pi}{2d})\ket{u_{d-1}}\right)
	\left( \sin(\frac{\pi}{2d})\bra{u_1} + \cos(\frac{\pi}{2d})\bra{u_{d-1}}\right).
\end{align*}
About Bob's projectors for input $y \in [2]+1$, we are only interested in their actions when restricted 
to the subspace $V$. Their actions on the subspace $V^\perp$ is irrelevant in this game.

The self-testing property of this correlation is summarized in the following lemma.
\begin{lemma}
	\label{lm:chsh_comp}
	Suppose a quantum strategy $\left(\{\{\tA_x^a\}_a\}_x, \{\{\tB_y^b\}_b\}_y, \ket{\tpsi}\right)$
	achieves the optimal correlation of the weighted $\CHSH$ test, 
	and let $\ket{\tpsi'} = \tA_\ast^\diamond \x \tB_\ast^\diamond \ket{\tpsi}/\norm{\tA_\ast^\diamond \x \tB_\ast^\diamond \ket{\tpsi}}^2$,
	then there exists isometries $\Phi_A$ and $\Phi_B$ 
	and a quantum state $\ket{junk}$
	such that 
	\begin{align*}
		&\Phi_A\x \Phi_B \left( \ket{\tpsi'}\right) = \ket{\EPR{2}}\x \ket{junk} \\
		&\Phi_A\x \Phi_B \left[\left(\1 \x  \frac{\tB_1 + \tB_2}{2\cos(\pi/d)}\right)\ket{\tpsi'}\right]
		=[(\1 \x \pauliz) \ket{\EPR{2}}] \x \ket{junk} \\
		&\Phi_A\x \Phi_B \left[\left(\1 \x  \frac{\tB_1 - \tB_2}{-2\sin(\pi/d)}\right)\ket{\tpsi'} \right]
		=[(\1 \x \paulix) \ket{\EPR{2}}] \x \ket{junk} 
	\end{align*}
	where $\tB_1 = \tB_1^0 - \tB_1^1$ and $\tB_2 = \tB_2^0 - \tB_2^1$.
\end{lemma}
\begin{proof}
Note that $\tA_x^a \tB_y^b$ means $\tA_x^a \x\tB_y^b$ in the following proof.

From the marginal distribution $\pb{\diamond}{\ast} = \pa{0}{1}+\pa{1}{1} = 2/(d-1)$,
we know $\| \tB_\ast^\diamond \ket{\tpsi} \| = \| (\tA_1^0+\tA_1^1) \ket{\tpsi}\| = \sqrt{2/d-1}$.
Since $\pr{0\diamond}{1\ast} + \pr{1\diamond}{1\ast} = 2/(d-1)$, we find that 
\begin{align*}
	\frac{\bra{\tpsi}\tB_\ast^\diamond (\tA_1^0+\tA_1^1) \tB_\ast^\diamond\ket{\tpsi}}{ \| \tB_\ast^\diamond \ket{\tpsi} \|^2} = 1,
\end{align*}
which means that 
\begin{align}
	(\tA_1^0+\tA_1^1)\tB_\ast^\diamond \ket{\tpsi} = \tB_\ast^\diamond \ket{\tpsi}.
\end{align}
Using the commutation relation between $(\tA_1^0+\tA_1^1)$ and $\tB_\ast^\diamond$, we get
\begin{align*}
	\frac{\bra{\tpsi} (\tA_1^0+\tA_1^1) \tB_\ast^\diamond(\tA_1^0+\tA_1^1)\ket{\tpsi}}{ \| (\tA_1^0+\tA_1^1) \ket{\tpsi} \|^2} = 1,
\end{align*}
Similar argument gives us that 
\begin{align}
	\tB_\ast^\diamond (\tA_1^0+\tA_1^1) \ket{\tpsi} = (\tA_1^0+\tA_1^1)\ket{\tpsi}.
\end{align}
The two equations above can be chained by commutativity to reach the conclusion that 
\begin{align}
	(\tA_1^0+\tA_1^1)\ket{\tpsi} = \tB_\ast^\diamond \ket{\tpsi}.
\end{align}
Following the same line of argument, we can conclude that
\begin{align}
	\tB_\ast^\diamond \ket{\tpsi} = \tA_\ast^\diamond \ket{\tpsi} = (\tA_1^0+\tA_1^1)\ket{\tpsi} = (\tA_2^0+\tA_2^1)\ket{\tpsi}.
\end{align}
Looking at the marginal distribution when Alice and Bob output $\perp$, we conclude that 
\begin{align}
	\tB_\ast^\perp \ket{\tpsi} = \tA_\ast^\perp \ket{\tpsi} = \tA_1^\perp \ket{\tpsi} = \tA_2^\perp \ket{\tpsi},
\end{align}
with similar arguments.

Next we examine the CHSH-type correlation when $x,y \in [2]+1$,
\begin{align*}
	   \bra{\tpsi} \tA_1^0\tB_1^0 \ket{\tpsi} 
	= &\bra{\tpsi}(\tA_\ast^\diamond + \tA_\ast^\perp) \tA_1^0\tB_1^0 (\tA_\ast^\diamond + \tA_\ast^\perp)\ket{\tpsi} \\
	= & \bra{\tpsi}\tA_\ast^\diamond \tA_1^0\tB_1^0 \tA_\ast^\diamond\ket{\tpsi} + \bra{\tpsi}\tA_\ast^\diamond \tA_1^0\tB_1^0 \tA_\ast^\perp\ket{\tpsi} \\
	&+\bra{\tpsi}\tA_\ast^\perp \tA_1^0\tB_1^0 \tA_\ast^\diamond\ket{\tpsi} + \bra{\tpsi}\tA_\ast^\perp \tA_1^0\tB_1^0 \tA_\ast^\perp\ket{\tpsi}\\
	= & \bra{\tpsi}\tB_\ast^\diamond \tA_1^0\tB_1^0 \tB_\ast^\diamond\ket{\tpsi} + \bra{\tpsi}\tA_\ast^\diamond \tA_1^0\tB_1^0 \tA_0^\perp\ket{\tpsi} \\
	&+\bra{\tpsi}\tA_0^\perp \tA_1^0\tB_1^0 \tA_\ast^\diamond\ket{\tpsi} + \bra{\tpsi}\tA_0^\perp \tA_1^0\tB_1^0 \tA_0^\perp\ket{\tpsi}\\
	=&\bra{\tpsi}\tB_\ast^\diamond \tA_1^0\tB_1^0 \tB_\ast^\diamond\ket{\tpsi},
\end{align*}
where we use the facts that $\tB_\ast^\diamond \ket{\tpsi} = \tA_\ast^\diamond \ket{\tpsi}$, $\tA_\ast^\perp \ket{\tpsi} = \tA_1^\perp \ket{\tpsi}$ and that 
$\spn(\tA_1^0) \cap \spn(\tA_0^\perp) = \emptyset$. 
This means that if Alice and Bob share the state $\tB_\ast^\diamond \ket{\tpsi}/\|\tB_\ast^\diamond \ket{\tpsi}\|$ and apply 
$\tA_1^0\tB_1^0$, the conditional probability is
\begin{align}
	\frac{\bra{\tpsi}\tB_\ast^\diamond \tA_1^0\tB_1^0 \tB_\ast^\diamond\ket{\tpsi}}{\bra{\tpsi} \tB_\ast^\diamond \tB_\ast^\diamond \ket{\tpsi}} = \frac{\cos^2(\pi/2d)}{2}.
\end{align} 
We can re-normalize the other correlations of $a,b \in [2]$ when $x,y \in [2+1]$ similarly, and get a new set of correlations 
which achieves the maximal value of $\langle \1_{-\cot(\pi/2d)}\rangle$. 
The conclusion of \cref{lm:chsh_comp} follows the application of \cref{thm:selftest} on the state $\tA_\ast^\diamond\tB_\ast^\diamond \ket{\tpsi}/\|\tA_\ast^\diamond\tB_\ast^\diamond \ket{\tpsi}\|$ as $\tA_\ast^\diamond\tB_\ast^\diamond \ket{\tpsi} = \tB_\ast^\diamond \ket{\tpsi}$.
\end{proof}
Note that the combination of \cref{lm:chsh_comp} with \cref{prop:2d-subspace} gives us that 
$\tB_1\tB_2$ has eigenvalues $\omega_d$ and $\omega_d^{-1}$.



%=====================================
\section{Main result}
\label{sec:main}
%=====================================
In this section, we introduce the correlation $C(\dr{r})$, which is the
ideal correlation of $\G(\dr{r})$, and then
prove that it can self-test the state $\ket{\EPR{d-1}}$.

%-----------------------------------------------------------------
\subsection{The correlation $C(\dr{r})$}
%-----------------------------------------------------------------
Recall that the linear system game $\LS_r$ has $n_r$ variables and $m_r$ equations.
In game $\G(\dr{r})$, 
Alice receives $x \in \{1,\dots m_r+3 \}$ and Bob receives
$y \in \{1,\dots,n_r+1\}$. We follow the previous structure by first give intuition about how they should
behave in this game. The correlation can be easily extracted from the behaviour list below.
\begin{itemize}
	\item When $x \in \{1,\dots m_r\}$ and $y \in \{1, \dots n_r\}$, they should win the 
	linear system game $\LS_r$ perfectly;
	\item when $x \in \{m_r+1, m_r+2, m_r+3\}$ and $y \in \{1, 2, n_r+1\}$, they should follow the
	optimal correlation of the test $\CHSH_X$, where 
	\begin{align}
		&\ast_A = m_r+1, \quad 0_A = m_r+2,\quad 1_A = m_r+3,\\
		&\ast_B = n_r+1,\quad 0_B = 1, \quad 1_A = 2,
	\end{align}
	are the inputs for the game $\CHSH_X$
	(The intuition behind is that $B_1B_2 = X$.);
	\item otherwise, their behaviour is irrelevant.
\end{itemize}
Note that the dimension $\dr{r}$ is defined in the rules of $\CHSH_X$.

\begin{proposition}
	\label{prop:realize}
	The correlation $C(\dr{r})$ can be realized by a quantum strategy.
\end{proposition}
We prove this proposition by giving the ideal strategy.
\begin{proof}
The construction start with the ideal strategy for the weighted $\CHSH_X$ test.
We choose $\ket{\psi} = \frac{1}{\sqrt{d-1}}\sum_{i=1}^{d-1} \ket{ii}$ and $V = \spn(\ket{1} ,\ket{d-1})$.
Alice's projectors and observables for the weighted $\CHSH_X$ test are
\begin{align*}
	&A_\ast^\diamond = \ketbra{1}{1} + \ketbra{d-1}{d-1}, &&A_\ast^\perp = \1 - A_\ast^\diamond, \\
	&A_1 = A_1^0-A_1^1 = \ketbra{1}{1} - \ketbra{d-1}{d-1}, &&A_1^\perp = A_\ast^\perp,\\
	&A_2 = A_2^0-A_2^1 = \ketbra{1}{d-1} + \ketbra{d-1}{1} ,&&A_2^\perp = A_\ast^\perp.
\end{align*}
Bob's observables restricted to the subspace $V$ is
\begin{align*}
	B_1|_V = \cos(\frac{\pi}{d})(\ketbra{1}{1} - \ketbra{d-1}{d-1}) - \sin(\frac{\pi}{d})(\ketbra{1}{d-1} + \ketbra{d-1}{1}),\\
	B_2|_V = \cos(\frac{\pi}{d})(\ketbra{1}{1} - \ketbra{d-1}{d-1}) +\sin(\frac{\pi}{d})(\ketbra{1}{d-1} + \ketbra{d-1}{1}).
\end{align*}
To make sure operator $B_2$ commutes with the operator $U$ that we will construct later, we pick a 
special basis of $X =B_1B_2$, which is
\begin{align}
	\ket{x_1} = \frac{-1}{\sqrt{2}}(\ket{1} + i\ket{d-1}), &&
	\ket{x_{d-1}} = \frac{-e^{i\pi/d}}{\sqrt{2}}(\ket{1} - i\ket{d-1}).
\end{align}
It can be checked that $X\ket{x_1} = \omega_d \ket{x_1}$ and $X\ket{x_{d-1}} = \omega_d^{d-1} \ket{x_{d-1}}$.
In this basis, $B_1$ and $B_2$ are expressed as 
\begin{align}
	&B_1|_V = \omega_d\ketbra{x_1}{x_{d-1}} + \omega_d^{-1} \ketbra{x_{d-1}}{x_1},\\
	&B_2|_V =\ketbra{x_1}{x_{d-1}}  + \ketbra{x_{d-1}}{x_1}.
\end{align}
Since Bob gets the same symbols $1$ and $2$ in different sub-tests, 
the observables $B_1$ and $B_2$ are extended from $B_1|_V$ and $B_2|_V$ as follows
%\begin{align}
%	B_1 = \ketbra{x_0}{x_0} + 
%	\sum_{k=1}^{(d-1)/2}\left( e^{ik\pi/2d}\ketbra{x_k}{x_{d-k}} + e^{-ik\pi/2d}\ketbra{x_{d-k}}{x_k}\right)\\
%	B_2 = \ketbra{x_0}{x_0} + 
%	\sum_{k=1}^{(d-1)/2}\left( e^{ik\pi/2d}\ketbra{x_{d-k}}{x_k} + e^{-ik\pi/2d}\ketbra{x_{k}}{x_{d-k}}\right)
%\end{align}
\begin{align}
	B_1 &= \sum_{k=1}^{(d-1)/2}\left( \omega_d^k\ketbra{x_k}{x_{d-k}} + \omega_d^{-k}\ketbra{x_{d-k}}{x_k}\right)\\
	B_2 &= \sum_{k=1}^{(d-1)/2}\left(\ketbra{x_{d-k}}{x_k} + \ketbra{x_k}{x_{d-k}}\right).
\end{align}
It can be checked that $B_1B_2 = \sum_{i=1}^{d-1} \omega_d^i \ketbra{x_i}{x_i}$ and the operator $U$ is
\begin{align}
	U = \sum_{i=1}^{d-1} \ketbra{x_{i/r \pmod{d}}}{x_i}.
\end{align}
The first step of the embedding procedure in the proof of \cref{prop:embed} requires
the commutativity between $U$ and $B_2$, which can be verified as follows,
\begin{align}
	UB_2U\ct = \sum_{i=1}^{d-1} U\ketbra{x_i}{x_{d-i}}U\ct = \sum_{i=1}^{d-1} \ketbra{x_{i/r \pmod{d}}}{x_{(d-i)/r \pmod{d}}}.
\end{align}
Let $i_1 = i/r \pmod{d}$ and $i_2  = (d-i)/r \pmod{d}$, then
\begin{align*}
	i_1 + i_2 \equiv& \frac{i}{r} + \frac{d-i}{r} \pmod{d} \\
	\equiv& \frac{d}{r} \pmod{d}.
\end{align*}
Suppose $d/r \pmod{d} \equiv x$ for some integer $x$, then
\begin{align}
	rx \equiv d \pmod{d}.
\end{align}
Since $d$ is a odd prime number and $r < d$, then $r$ cannot divide $d$, so $x = 0$ or $d$.
Considering the fact that $1 \leq i_1 \leq d-1$ and $1 \leq i_2 \leq d-1$, we know $x = i_1+i_2 = d$.
Hence, conjugation by $U$ will transform $\ketbra{x_i}{x_{d-i}}$ into another term of the same form.
Now suppose for $1 \leq i < i' \leq d-1$ and $U\ketbra{x_i}{x_{d-i}}U\ct = U\ketbra{x_i'}{x_{d-i'}}U\ct$,
it means that $i/r \equiv i'/r \pmod{d}$, which further implies that $i = i'$.
To conclude, conjugation by $U$ only permutes the terms of $B_2$, so
\begin{align}
	UB_2U\ct = B_2.
\end{align}
%There are two cases to consider which are the index is dividable by $r$ or not.
%Note that in the following calculation, the division is not with respect to the modulo $d$.
%When the index $i \equiv 0 \pmod{r}$, $U\ketbra{x_i}{x_{d-i}}U = \ketbra{x_{i/2}}{x_{d-i/r}}$, and
%when the index $i \not\equiv 0 \pmod{r}$, $U\ketbra{x_i}{x_{d-i}}U = \ketbra{x_{(d+i)/2}}{x_{(d-i)/2}}$,
%so we can calculate that 
%\begin{align}
%	\sum_{i=1}^{d-1} U\ketbra{x_i}{x_{d-i}}U\ct 
%	=& \sum_{i=1}^{(d-1)/2}U\ketbra{x_{2i}}{x_{d-2i}}U\ct 
%	+ \sum_{i=0}^{(d-1)/2-1} U\ketbra{x_{2i+1}}{x_{d-(2i+1)}}U\ct\\
%	=& \sum_{i=1}^{(d-1)/2}\ketbra{x_i}{x_{d-i}} + \sum_{i=0}^{(d-1)/2-1} \ketbra{x_{(d+1)/2+i}}{x_{(d-1)/2-i}}\\
%	=& \sum_{i=1}^{(d-1)/2}\ketbra{x_i}{x_{d-i}} + \sum_{i=(d+1)/2}^{d-1} \ketbra{x_i}{x_{d-i}}\\
%	=&\sum_{i=1}^{d-1} \ketbra{x_i}{x_{d-i}}.
%\end{align}
The decomposition of $U$ into $B_3$ and $B_4$ is similar to the decomposition of $X$.
Suppose the eigen-decomposition of $U$ is
\begin{align}
	U = \sum_{i=0}^{d-1} \omega_{d-1}^i \ketbra{u_i}{u_i},
\end{align} 
then the decomposition is the following 
\begin{align}
	B_3 =& \ketbra{u_0}{u_0} +\omega_{d-1}^{(d-1)/2}\ketbra{u_{(d-1)/2}}{u_{(d-1)/2}} + \sum_{k=1}^{(d-3)/2}\left( \omega_d^k\ketbra{u_k}{u_{d-1-k}} + \omega_d^{-k}\ketbra{x_{d-1-k}}{x_k}\right)\\
	B_4 &= \ketbra{u_0}{u_0} +\ketbra{u_{(d-1)/2}}{u_{(d-1)/2}} + \sum_{k=1}^{(d-3)/2}\left(\ketbra{u_{d-1-k}}{u_k} + \ketbra{u_k}{u_{d-1-k}}\right).
\end{align}

In the next step of embedding, for each $x_i$, we find $y_i$ ,$z_i$, $w_i$ and $f$ such that
$x_i = y_iz_i = fw_i$. We demonstrate how this splitting is done by pick $x_1$ as an example, 
which is mapped to $B_1$ in the previous step. Now we map $x_1$ to $B_1'$ which is
\begin{align}
	B_1' = \begin{pmatrix}
	B_1 & 0 \\
	0 & B_1
	\end{pmatrix},
\end{align}
then $y_1$, $z_1$, $w_1$ and $f$ are mapped to
\begin{align}
y_1 \to 
\begin{pmatrix}
B_1 & 0\\
0 & \1
\end{pmatrix},
&&
z_1 \to
\begin{pmatrix}
\1 & 0\\
0 & B_1
\end{pmatrix},
\\
w_1 \to 
\begin{pmatrix}
0 & B_1\\
B_1 & 0
\end{pmatrix},
&&
f \to
\begin{pmatrix}
0 & \1\\
\1 & 0
\end{pmatrix}.
\end{align}

The last step of embedding follows the same manner as we double the dimension of the operators again.
For example, $y_1$ is mapped to $B_1 \oplus \1 \oplus B_1 \oplus \1$ and $z_1$ is mapped to 
$\1 \oplus B_1 \oplus \1 \oplus B_1$, so effectively, $x_1$ is mapped to $B_1^{\oplus 4}$.
\footnote{Details of this step of embedding can be found in proof of Proposition~$4.2$ in Ref.~\cite{slofstra2017}.}

In the end, we determine the shared state.
The optimal correlation for $\CHSH_X$ uses state 
$\ket{\psi} = \frac{1}{\sqrt{d-1}} \sum_{i=1}^{d-1} \ket{u_i}\ket{u_i}$, 
so the shared state for $\G$ is $\frac{1}{2}( \ket{\psi}^{\oplus 4})$.
\end{proof}
Next we are going to prove that the correlation $C(\dr{r})$, which is produced by the 
strategy winning this game optimally, can self-test $d-1$-dimensional EPR pair.

%-----------------------------------------------------------------
\subsection{Self-test}
%-----------------------------------------------------------------
\hl{This subsection needs to be formalized.}
We give a formal version of \cref{thm:pr_2} first and then prove it.
\begin{theorem}
\label{thm:selftest}
	If a quantum strategy using the shared state $\ket{\psi}$ achieves the ideal 2-party correlation $C(\dr{r})$ where $d$ is an odd
	prime number with primitive root $r \in \{2,3,5\}$, then there exist local isometries $\Phi_A$ and $\Phi_B$, and a state $\ket{junk}$ such 
	that $\Phi_A\x\Phi_B \ket{\psi} = \ket{\EPR{d-1}} \x \ket{junk}$.
\end{theorem}
\begin{proof}
Suppose Alice and Bob achieve the optimal correlation with the quantum strategy $(\ket{\psi}, \{A_x\}, \{B_y\})$
for all $x,y \in \calX \times \calY$ .
The observables $A_x$ and $B_y$ and the shared state $\ket{\psi}$ shall not be confused with the ones used in the optimal strategy.
By Lemma~$4.3$ of Ref.~\cite{coladan2017}, we can extract an operator solution from the perfect winning strategy 
of the linear system game $\LS_r$. 
For each variable $\{ x_i \}_{i=1}^{n_r}$, Alice and Bob has operators $A_i$ and $B_i$ respectively.
The condition that they agree with assignment to variables means that 
\begin{align}
	\bra{\psi} A_i \otimes \overline{B_i} \ket{\psi} = 1 \Rightarrow A_i \otimes \overline{B_i} \ket{\psi} = \ket{\psi}
	\text{ for } 1 \leq i \leq n_r
\end{align}
and the condition that Alice's assignments satisfy the constraint means that 
\begin{align}
	\Tr(\rho_A \Pi_{j: H(i,j) \neq 0} A_j) = \Tr(\rho_A) \text{ for all } 1 \leq i \leq m_r
\end{align}
where $\rho_A =  \Tr_B(\ketbra{\psi}{\psi})$. 
Similarly we define $\rho_B = \Tr_A(\ketbra{\psi}{\psi})$.
For any $\ket{v} \in \supp(\rho_A)$,
we have 
\begin{align}
\Pi_{j:H(i,j) \neq 0} A_j \ket{v} = \ket{v} \text{ for all } 1 \leq i \leq m_r.
\end{align}
Since the relation $uxu^{-1} = x^r$, where $r$ is the primitive root of $d$, is embedded in this linear system game, we know
\begin{align}
	A_3A_4 A_1A_2 (A_3A_4)^\dagger \ket{v}= (A_1A_2)^r \ket{v} \text{ for all } \ket{v} \in \supp(\rho_A).
\end{align}
For simplicity, we define $X_A = A_1A_2$ and $U_A=A_3A_4$ such that
the condition is equivalent to
\begin{align}
	\label{eq:ux_relation}
	U_AX_AU_A^\dagger \ket{v} = X_A^r \ket{v} \text{ for all } \ket{v} \in \supp(\rho_A).
\end{align}
We will come back to the implication of this condition later.

Suppose $A_{m_r+1} = A_\ast^\diamond- A_\ast^\perp = \Pi_{V_A} - \Pi_{V_A^\perp}$m, where $V_A$ is a
$2m$-dimensional vector space and $\Pi_{V_A}$ is the projector onto it. The reason why it has dimension $2m$
will be clear shortly. On Bob's side, we also have $B_{n_r+1} = B_\ast^\diamond- B_\ast^\perp = \Pi_{V_B} - \Pi_{V_B^\perp}$.
Recall that from the extended weighted CHSH test, we know
\begin{align}
	A_\ast^\diamond \ket{\psi} = B_\ast^\diamond \ket{\psi} = A_\ast^\diamond \x B_\ast^\diamond \ket{\psi},
\end{align}
which implies that with an appropriate change of basis, we can get $V_A = V_B$, so in the rest of the proof
we drop the subscript of $V$.
By \cref{lm:chsh_comp} and \cref{prop:2d-subspace}, we know $V$ consists of $\omega_d$-eigenvectors and $\omega_d^{-1}$ eigenvectors of 
$X_A$, so we can write 
\begin{align}
	\Pi_{V} = \Pi_{V_1} + \Pi_{V_{d-1}},
\end{align}
where $\Pi_{V_{1}}$ is the projector onto the $\omega_d$-eigenspace of $X_A$ and $\Pi_{V_{d-1}}$ is the projector 
onto the $\omega_d^{-1}$-eigenspace of $X_A$.
Suppose $\ket{x_{1}} \in V_{1}$, then $X_A \ket{x_{1}} = \omega_d \ket{x_{1}}$.
By \cref{eq:ux_relation} we can calculate that
\begin{align}
\label{eq:ladder}
 X_AU_A^\dagger \ket{x_{1}} = U_A^\dagger X_A^r \ket{x_{1}} = \omega_d^r U_A^\dagger \ket{x_{1}},
\end{align}
so $U_A^\dagger \ket{x_{1}}$ is an eigenvector of $X_A$ with eigenvalue $\omega_d^r$.
By induction, we know $X_A (U_A^\dagger)^i \ket{x_{1}} = \omega_d^{r^i} (U_A^\dagger)^i\ket{x_{1}}$. 
From the set $\{(U_A^\dagger)^i \Pi_{V_1} (U_A)^i \}_{i=0}^{d-2}$, we can identify $\Pi_{V_i}$ for $i = 1 \dots  d-1$
such that $\Pi_{V_i}$ is the projector onto the $\omega_d^i$-eigenspace of $X_A$,
and 
\begin{align}
 \cup_{i \in [d-1]+1} V_i \subset \supp(\rho_A).
\end{align}
Since unitary transformation does not change the rank of a matrix, we know $\rank(\Pi_{V_1}) = \rank(\Pi_{V_{i}}) =N$
for $ i =1 \dots d-1$.
We pick a basis for $V_1$ such that 
\begin{align}
	\Pi_{V_1} =  \sum_{j=1}^m \ketbra{x_{1,j}}{x_{1,j}},
\end{align}
then we can construct 
\begin{align}
 \Pi_{V_i} = \sum_{j=1}^m \ketbra{x_{i,j}}{x_{i,j}},
\end{align}
where $\ket{x_{i,j}} = (U\ct)^{k_i} \ket{x_{1,j}}$ for $r^{k_i} \equiv i \pmod{d}$ and $1 \leq j \leq m$.
By \cref{eq:ux_relation} we also know that $U_A \ket{x_{i,j}} = \ket{x_{i/2,j}}$ for $i = 1,2 \dots d-1$.
In order to apply \cref{lm:ux_independ}, we construct $m$ subspaces $\{W_j\}_{j=1}^m$ where 
\begin{align}
	W_j = \spn( \{ \ket{x_{i,j}} \}_{i=1}^{d-1} )
\end{align}
The subspace $W_j$ is orthogonal to $W_{j'}$ for $j \neq j'$, and
$U_A$ and $X_A$ satisfy the condition of \cref{lm:ux_independ} when their actions are 
restricted to each $W_j$.
Similar argument also applies to operator $X_B$ and $U_B$ on Bob's side.

%Similar argument applies to Bob's operators. We can identify $X_B = B_1B_2$, 
%$U_B=B_3B_4$ and a vector space $S_B =\spn\{ \ket{x_{B,1}}, \ket{x_{B,2}} \dots  \ket{x_{B,d-1}} \} \subset \supp(\rho_B)$,
%such that 
%\begin{align}
%	X_B = \sum_{i=1}^{d-1} \omega_d^i \ketbra{x_{B,i}}{x_{B,i}} && U_B = \sum_{i=1}^{d-1} \ketbra{x_{B,i/2}}{x_{B,i}},
%\end{align}
%when their actions are restricted to $S_B$. Moreover, we can define a unitary $V: \supp{\rho_B} \to \supp(\rho_A)$,
%which maps $\ket{x_{B,i}}$ to $\ket{x_{A,i}}$ for $i=1\dots d-1$.


%and projectors $\Pi_{S_A}$ and $\Pi_{S_B}$ to be the projectors onto $S_A$ and $S_B$ respectively.
%From now on, we focus on the state $\ket{\psi_S}$ which is defined as
%\begin{align}
%	\ket{\psi_S} = (\Pi_{S_A} \x (V\Pi_{S_B}))\ket{\psi} = \vc(\tau) 
%\end{align}
%for some $\tau \in L(S_A)$.
With an appropriate change of basis, we 
assume that $\ket{\psi} = \vc(\tau)$ for some $\tau \in L(\supp(\rho_A))$
The consistency condition is equivalent to
\begin{align}
\label{eq:con_tau}
	A_i \tau B_i^\dagger = \tau.
\end{align}
Substituting $i=1,2$ into \cref{eq:con_tau}, we get 
\begin{align}
	X_A \tau X_A^\dagger = A_1A_2\tau B_2^\dagger B_1^\dagger = A_1\tau B_1^\dagger = \tau.
\end{align}
Similar argument gives us that 
\begin{align}
	U_A \tau U_A^\dagger = \tau.
\end{align}
Then we can conclude that for any $k \in \{0,1 \dots d-2\}$ and $l \in \{1,2\dots d-1\}$
\begin{align}
	U_A^kX_A^l \tau (U_A^kX_A^l)^\dagger = \tau.
\end{align}
Let $\Pi_{W_j}$ be the projector onto $W_j$. 
By \cref{lm:ux_independ}, $\Pi_{W_j} \tau \Pi_{W_j}$ commutes with all the $(d-1) \times (d-1)$ matrices, which means that 
\begin{align}
	\label{eq:d-1}
	\Pi_{W_j} \tau \Pi_{W_j}  = c_j \1_{W_j} \text{ for } j = 1\dots m.
\end{align}
In the vector form, we know
\begin{align}
	\sum_{j =1}^N \Pi_{W_j} \ket{\psi} =\sum_{i=1}^{d-1} \sum_{j=1}^m c_j \ket{x_{i,j}}\ket{x_{i,j}}. 
\end{align}
Recalling the fact that 
\begin{align}
\norm{ \Pi_{V_1} + \Pi_{V_{d-1}} \ket{\psi}}^2 = \norm{\Pi_{V_1} \ket{\psi}}^2 + \norm{\Pi_{V_{d-1}} \ket{\psi}}^2 = \frac{2}{d-1},
 \end{align}
 it means that 
 \begin{align}
 	2 \sum_{j=1}^m \norm{c_j}^2 = \frac{2}{d-1}.
 \end{align}
 Then we can calculate the norm of $\sum_{j =1}^m \Pi_{W_j} \ket{\psi}$, which is
 \begin{align}
 \norm{\sum_{j =1}^m \Pi_{W_j} \ket{\psi}}^2 = \sum_{i=1}^{d-1} \sum_{j=1}^m \norm{c_j}^2 = 1 = \norm{\ket{\psi}}^2.
 \end{align}
 We can conclude that $\supp{\rho_A} = \cup_{i=1}^{d-1} V_i$ and 
 \begin{align}
 	\ket{\psi} = \sum_{i=1}^{d-1}\sum_{j=1}^m c_j \ket{x_{i,j}}\ket{x_{i,j}}.
 \end{align}
 
The last step is to construct the local isometries $\Phi_A$ and $\Phi_B$ that can produce $\ket{\EPR{d-1}}$ from $\ket{\psi}$.
We give steps about how $\Phi_A$ and $\Phi_B$ works.
Since the subspaces $W_j$'s are orthogonal, Alice and Bob can map state $\ket{\psi}$ to the state 
\begin{align}
	\ket{\psi'} = \sum_{i=1}^{d-1}\sum_{j=1}^m c_j \ket{x_{i,j}}_A\ket{x_{i,j}}_B \ket{j}_A\ket{j}_B.
\end{align}
Note that we added subscript $A$ and $B$ to stress the subsystems holden by Alice and Bob respectively.
Since $\ket{x_{i,j}}$ is orthogonal to $\ket{x_{i',j'}}$ for $i\neq i'$ or $j \neq j'$, we further assume that there exists 
a unitary $O$ such that $O\ket{x_{i,j}} = O\ket{x_{i,j+1}}$ for all $i$ and $j$, but when $j = N$, $O\ket{x_{i,N}} = \ket{x_{i,1}}$.
Then controlled by the appended register, Alice and Bob can apply $(O\ct)^j$ to the original system
and $\ket{\psi'}$ is mapped to
\begin{align}
	 &\sum_{i=1}^{d-1}\sum_{j=1}^m c_j [(O\ct)^j\ket{x_{i,j}}_A] \x [(O\ct)^j\ket{x_{i,j}}_B] \x \ket{j}_A\ket{j}_B\\
	 = &\sum_{i=1}^{d-1}\sum_{j=1}^m c_j \ket{x_{i,1}}_A\ket{x_{i,1}}_B \ket{j}_A\ket{j}_B\\
	 =  &\frac{1}{\sqrt{d}} \sum_{i=1}^{d-1} \ket{x_{i,1}}_A\ket{x_{i,1}}_B \x \sum_{j=1}^m \sqrt{d}c_j \ket{j}_A\ket{j}_B
\end{align}
After relabelling $\ket{x_{i,1}}$ as $\ket{i}$, we factor $\ket{\EPR{d-1}}$ out of $\Phi_A \x \Phi_B \ket{\psi}$.
\end{proof}
%The effect of $\CHSH_X$ and $\SVT_X$ is that we have a set $\{ \ket{x_i} \}_{i \in [d]} \in \supp(\rho_A)$ such that
%\begin{align}
%	X \ket{x_i} = \omega_d^i \ket{x_i}.
%\end{align}
%The other effect of $\SVT_X$ is that we know 
%\begin{align}
%	\tA_\triangle^\diamond \rho_A \tA_\triangle^\diamond = \frac{1}{d} \ketbra{x_0}{x_0} = \frac{1}{d} \tA_\triangle^\diamond.
%\end{align}
%\hl{\textbf{Question}: Can we show $\Tr(\ketbra{x_2}{x_2} \rho_A) = 1/d$?}
In summary, for any odd prime number $d$ whose primitive root is $2$,$3$ or $5$, 
\cref{prop:realize} tells us that the correlation $C(d)$ is achievable by a quantum strategy 
and \cref{thm:selftest} tells us that the correlation $C(d)$ can self-test the EPR pair of
local dimension $d-1$. Moreover, there are infinitely many prime numbers
whose primitive root is in the set $\{2,3,5\}$ \cite{murty1988},
Hence, our main result is the following theorem.
\begin{theorem}
	There exists an infinity-sized set $D$ of prime numbers such that 
	each $d \in D$ has primitive root $2$, $3$ or $5$ and there exists
	a constant-sized correlation $C(d)$ that can self-test the EPR pair of 
	local dimension $d-1$.
\end{theorem}
We remark that our proof works for any odd prime number with primitive root $r$.
However, since there is no upper-bound of $r$ for a general prime number $d$, we 
cannot claim the corresponding correlation $C(d)$ is constant-sized. On the other
hand, since $r$ is usually much smaller than $d$, our result implies a more efficient
way to test EPR pairs of prime local dimension than the one proposed in Ref.~\cite{cgs2017}.
The size of our correlation grows linearly in $r$ whereas the Coladangelo \textit{et. al.}'s
method uses correlation grows linearly in $d$.

\bibliographystyle{alphaurl}
\bibliography{quantum_correlation}
\appendix
%========================================
\section{Proof of \cref{thm:selftest} }
\label{sec:selftest}
%========================================
\begin{proof}
Following the techniques developed in Ref.~\cite{bamps2015}, the first step is to find a sum-of-square decomposition of 
\begin{align}
	\bar{\I}_\alpha = 2\sqrt{\alpha^2+1} \1 - \I_\alpha
	= \frac{2}{\sin(\mu)} \1 - \frac{\cos(\mu)}{\sin(\mu)}(A_1B_1+A_1B_2) -  A_2B_1 + A_2B_2.
\end{align} 
With the following notation
\begin{align*}
	Z_A = A_1 &\quad X_A = A_2\\
	Z_B = \frac{B_1+B_2}{2\cos(\mu)} &\quad X_B = \frac{B_1-B_2}{2\sin(\mu)},
\end{align*}
the two SOS decompositions that we use are
\begin{align}
	\label{eq:sos1}&\bar{\I}_\alpha = \frac{\sin(\mu)\bar{\I}_\alpha^2 + 4\sin(\mu)\cos(\mu)^2(Z_AX_B+X_AZ_B)^2}{4},\\
	\label{eq:sos2}&\bar{\I}_\alpha = \frac{\cos^2(\mu)}{\sin(\mu)}(Z_A-Z_B)^2 + \sin(\mu) (X_A-X_B)^2.
\end{align}
The verification is omitted here.

Suppose the quantum strategy $(\ket{\psi}, \{\tilde{A}_x\}_{x \in [2]}, \{\tilde{B}_{y \in [2]}\}$ achieves that 
$\bra{\psi} \bar{\I}_\alpha \ket{\psi} \leq \epsilon$.
The second step is to establish bounds of the following form
\begin{align}
	&\|(\tilde{Z}_A-\tilde{Z}_B)\ket{\psi}\| \leq c_1 \sqrt{\epsilon}\\
	&\|(\tilde{X}_A(\1+\tilde{Z}_B)-\tilde{X}_B(\1-\tilde{Z}_A))\ket{\psi}\| \leq c_2 \sqrt{\epsilon}\\
	&\|(\tilde{X}_A-\tilde{X}_B)\ket{\psi}\| \leq c_3 \sqrt{\epsilon}\\
	&\|(\tilde{Z}_A\tilde{X}_A+\tilde{X}_A\tilde{Z}_A)\ket{\psi}\| \leq c_4 \sqrt{\epsilon}.
\end{align}
Now we write $s = \sin(\mu)$, $c = \cos(\mu)$ and define
\begin{align*}
	S_1 &= \frac{\sqrt{s}}{2} \bar{\I}_\alpha, \quad
	S_2 = \sqrt{s}c(\tilde{Z}_A\tilde{X}_B+\tilde{X}_A\tilde{Z}_B),\\
	S_3 &= \frac{c}{\sqrt{s}}(\tilde{Z}_A-\tilde{Z}_B),\quad
	S_4 = \sqrt{s}(\tilde{X}_A-\tilde{X}_B)
\end{align*}
then $\bar{\I}_\alpha = S_1^2 + S_2^2 = S_3^2 + S_4^2$ and $\bra{\psi}\bar{\I}_\alpha \ket{\psi} \leq \epsilon$ implies that 
$\bra{\psi}S^2_i \ket{\psi} \leq \epsilon$ and $\|S_i \ket{\psi} \| \leq \sqrt{\epsilon}$ for $i = 1,2,3,4$.
We can easily check that 
\begin{align*}
	c_1 = \frac{\sqrt{s}}{c}, \quad
	c_2 = \frac{1}{\sqrt{s}} + \frac{1}{c\sqrt{s}}, \quad
	c_3 = \frac{1}{\sqrt{s}}.
\end{align*}
where we use the relation that $\tilde{X}_A(\1+\tilde{Z}_B)-\tilde{X}_B(\1-\tilde{Z}_A) = S_4/s^{1/2} + S_2/(cs^{1/2})$.
To calculate $c_4$, we use the relation
\begin{align}
	\tilde{Z}_A\tilde{X}_A + \tilde{X}_A\tilde{Z}_A = \frac{S_2}{c\sqrt{s}} + \frac{\sqrt{s}\tilde{X}_AS_3}{c} + \frac{\tilde{Z}_AS_4}{\sqrt{s}}
\end{align}
and reach the conclusion that  
\begin{align}
	c_4 = \frac{1+c+s}{c\sqrt{s}}
\end{align}
where we use that fact that $\tilde{Z}_A, \tilde{X}_A$ are unitaries.

With appropriate substitutions, the rest of the proof follows the same derivation as that in Appendix A of Ref.~\cite{bamps2015},
so we omit it here. The dependence of the error term on $\alpha$ comes from the fact that 
\begin{align}
\frac{1}{\cos(\arctan(1/\alpha)) \sin^{1/2}(\arctan(1/\alpha))} = \frac{1}{\alpha}+\frac{3}{4}\alpha - O(\alpha^3).
\end{align}

\end{proof}
%%=====================================
\section{Equations to embed $uxu^{-1} = x^2$}
\label{sec:power2}
%%=====================================
\input{power2_equations}

%%=====================================
\section{Equations to embed $uxu^{-1} = x^3$}
\label{sec:power3}
%%=====================================
\input{power3_equations}

%%=====================================
\section{Equations to embed $uxu^{-1} = x^5$}
\label{sec:power5}
%%=====================================
\input{power5_equations}

\end{document}
