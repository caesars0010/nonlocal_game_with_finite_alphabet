\documentclass[11pt,letterpaper]{article}

\makeatletter
\renewcommand\paragraph{\@startsection{paragraph}{4}{\z@}%
                                      {1ex \@plus1ex \@minus.2ex}%
                                      {-1em}%
                                      {\normalfont\normalsize\bfseries}}
\makeatother

%%%%%%%%%%%%%%%%%%%%%
%  P A C K A G E S  %
%%%%%%%%%%%%%%%%%%%%%

% Authors
\usepackage{authblk}

% Page margins
\usepackage[margin=1in]{geometry}

% Nicer math font
\usepackage{mathpazo}

% More fancy lists
\usepackage{enumerate}

% Microtype
\usepackage{microtype}

% TikZ
\usepackage{tikz}
%\usetikzlibrary{calc,shapes.geometric}
\usetikzlibrary{backgrounds,fit,decorations.pathreplacing,calc}

% Highlights
\usepackage{soul}

% Young Tableaux
\usepackage{ytableau}

% Figure
\usepackage{float}

% Hypertext package
\usepackage[colorlinks = true]{hyperref}
% Title and authors
%\hypersetup{
%  pdftitle = {},
%  pdfauthor = {}
%}
% Color definitions
\definecolor{darkred}  {rgb}{0.5,0,0}
\definecolor{darkblue} {rgb}{0,0,0.5}
\definecolor{darkgreen}{rgb}{0,0.5,0}
% Color links
\hypersetup{
  urlcolor   = blue,         % color of external links
  linkcolor  = darkblue,     % color of internal links
  citecolor  = darkgreen,    % color of links to bibliography
  filecolor  = darkred       % color of file links
}

% AMS
\usepackage{amsmath,amssymb,amsfonts,amsthm,amstext}

%% Restating theorems
%\usepackage{thm-restate}

% Powerful macros
\usepackage{etoolbox}

% Fixes for amsmath
\usepackage{mathtools}
\mathtoolsset{centercolon}
\makeatletter
\protected\def\tikz@nonactivecolon{\ifmmode\mathrel{\mathop\ordinarycolon}\else:\fi}
\makeatother

% Daw boxes
\usepackage{tcolorbox}

% Code
\usepackage{algorithm}
%\usepackage{algorithmic}
\usepackage{algpseudocode}

% Clever references
\usepackage{cleveref}%[nameinlink]
\crefname{lemma}{Lemma}{Lemmas}
\crefname{proposition}{Proposition}{Propositions}
\crefname{definition}{Definition}{Definitions}
\crefname{theorem}{Theorem}{Theorems}
\crefname{conjecture}{Conjecture}{Conjectures}
\crefname{corollary}{Corollary}{Corollaries}
\crefname{claim}{Claim}{Claims}
\crefname{section}{Section}{Sections}
\crefname{appendix}{Appendix}{Appendices}
\crefname{figure}{Fig.}{Figs.}
\crefname{table}{Table}{Tables}
% \crefname{algorithm}{Algorithm}{Algorithms}

% IEEE tools
\usepackage[retainorgcmds]{IEEEtrantools}

% table of contents
\usepackage{tocloft}

% Table with multi-row
\usepackage{multirow}

%%%%%%%%%%%%%%%%%%%%%%%%%
%  N E W C O M M A N D  %
%%%%%%%%%%%%%%%%%%%%%%%%%

% Standard quantum notation

\newcommand{\ket}[1]{|#1\rangle}
\newcommand{\bra}[1]{\langle#1|}
\newcommand{\braket}[2]{\langle#1|#2\rangle}
\newcommand{\ketbra}[2]{|#1\rangle\langle#2|}
\newcommand{\proj}[1]{|#1\rangle\langle#1|}

\newcommand{\x}{\otimes}
\newcommand{\xp}[1]{^{\otimes #1}}
\newcommand{\op}{\oplus}

\newcommand{\ct}{^{\dagger}}
\newcommand{\tp}{^\intercal}

% Linear algebra

%\newcommand{\1}{\mathbb{1}} % identity matrix
%\DeclareMathOperator{\Hom}{Hom}
\DeclareMathOperator{\End}{End}
%\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\Lin}{L} % all linear maps
\newcommand{\Mat}[1]{\mathrm{M}(#1)} % all matrices
%\newcommand{\Mat}[1]{\mathrm{M}_{#1}(\C)}

% Paired delimiters

\DeclarePairedDelimiter{\set}{\lbrace}{\rbrace}
\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
\DeclarePairedDelimiter{\of}{\lparen}{\rparen}
\DeclarePairedDelimiter{\sof}{\lbrack}{\rbrack}
\DeclarePairedDelimiter{\ip}{\langle}{\rangle}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}

% Operators

\renewcommand{\Re}{\operatorname{Re}}
\renewcommand{\Im}{\operatorname{Im}}
\DeclareMathOperator{\vc}{vec}
\DeclareMathOperator{\spn}{span}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\spec}{spec}
\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\hook}{hook}
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\supp}{supp}


% Sets

\newcommand{\C}{\mathbb{C}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\calH}{\mathcal{H}}
\newcommand{\calX}{\mathcal{X}}
\newcommand{\calY}{\mathcal{Y}}
\newcommand{\calA}{\mathcal{A}}
\newcommand{\calB}{\mathcal{B}}


% Identity operator
\newcommand{\1}{\mathbb{1}}

% Pauli Group
\newcommand{\Pg}{\mathcal{P}}
\newcommand{\J}{\mathcal{J}}

% Special notation

\newcommand{\CHSH}{CHSH^{(d)}}
\newcommand{\MS}{MS}
\newcommand{\SVT}{SVT}
\newcommand{\EPR}[1]{EPR^{(#1)}}
\newcommand{\paulix}{\sigma_x}
\newcommand{\pauliz}{\sigma_z}
\newcommand{\G}{G}
\newcommand{\LS}{LS}
\newcommand{\tA}{\tilde{A}}
\newcommand{\tB}{\tilde{B}}
\newcommand{\tW}{\tilde{W}}
\newcommand{\tx}{\tilde{x}}
\newcommand{\tpsi}{\tilde{\psi}}
\newcommand{\tri}{\Delta}
\newcommand{\lB}{\overline{B}}


% Probabilities
\newcommand{\pr}[2]{P(#1|#2)}
\newcommand{\pa}[2]{P_A(#1|#2)}
\newcommand{\pb}[2]{P_B(#1|#2)}

% Bell Ineqaulities
\newcommand{\I}{\mathcal{I}}



%%%%%%%%%%%%%%%%%%%%%%%%%
%  N E W T H E O R E M  %
%%%%%%%%%%%%%%%%%%%%%%%%%

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{claim}[theorem]{Claim}
\newtheorem*{conjecture*}{Conjecture}
\newtheorem*{problem}{Problem}
\newtheorem*{example}{Example}

\theoremstyle{definition}
\newtheorem*{remark}{Remark}



%%%%%%%%%%%%%%%%
%   Document   %
%%%%%%%%%%%%%%%%

\begin{document}

\title{Self-test prime dimensional EPR pairs with constant alphabet}

\author[1]{Honghao Fu}
\author[1,2]{Carl Miller}

\renewcommand\Affilfont{\itshape\small}


\affil[1]{Department of Computer Science, Institute for Advanced Computer Studies, and Joint Center for Quantum \break Information and Computer Science, University of Maryland, College Park, MD 20742, USA}
\affil[2]{National Institute of Standards and Technology, 100 Bureau Dr., Gaithersbug, MD 20899, USA}
\maketitle

%========================================
\section{Introduction}
\label{sec:intro}
%========================================
Self-testing is a unique phenomenon of quantum mechanics. It has many applications in quantum
delegated computation \cite{ruv2013,cgsv2017} and device independent quantum cryptography
\cite{qkd2011,qkd2014,miller2016,fu2018,eat2018}.

The case of self-testing $2$-dimensional EPR pair is fully understood. One can robustly self-test
one copy of it by the CHSH inequality \cite{bamps2015}. and self-test many copies of the $2$-dimensional EPR
pair in parallel \cite{mckague2016, coladan2017parallel}. 
Self-testing general $d$-dimensional EPR pairs is a harder task.
Recently, a remarkable result by Coladangelo \textit{et al}.\cite{cgs2017} 
has shown that the maximally entangled state with arbitrary local dimension 
can be self-tested with constant question alphabet but answer alphabet growing with
the local dimension. 
Then Coladangelo and Stark \cite{coladan2017} further showed that by playing many instances
of the generalized Magic Square game and Magic Pentagram game, one can self-test
$N$ copies of the maximally entangled state with local dimension $d$ for any $d, N \geq 2$.
Both the Magic Square game and the Magic Pentagram game have constant question alphabet
but growing answer alphabet.

In summary, general $d$-dimensional maximally entangled states are self-tested by
modifying the correlation and enlarging the size of the correlation.
A natural question to ask is whether maximally entangled state with large local dimension
can be self-tested with fixed-sized correlation. 
An equivalent question to ask is whether it is possible to self-test maximally
entangled state more efficiently, with constant question and answer set. 
In this report, we give an affirmative answer to this question by proving the following theorem.
\begin{theorem}
\label{thm:inf}
	There exists an infinite-sized set $D$ of odd prime numbers such that, for any $d \in D$, 
	the maximally entangled state of local dimension $d-1$ can be self-tested 
	with constant-sized question and answer alphabets.
\end{theorem}

The set $D$ is easily characterizable as it contains all the odd prime numbers with 
primitive root $2$, $3$ or $5$. It has been shown that there are infinitely many prime numbers
with primitive root in the set $\{2,3,5\}$ \cite{murty1988}, so the set $D$ has infinitely many elements.
To prove \cref{thm:inf}, we give explicit self-testing proof of 
the maximally entangled state with local dimension $d-1$ where the primitive root of $d$ is $2$ by explicitly giving the 
correlation that achieves this goal. Then the self-testing property of this correlation is 
given in the following theorem.
\begin{theorem}
\label{thm:pr_2}
	All maximally entangled state with local dimension $d-1$, where $d$ is prime and has
	primitive root $2$, can be self-tested by a constant-sized correlation.
\end{theorem}
Note that although the sizes of the question and answer
sets do not depend on $d$, the optimal correlation does.
This correlation can be easily converted to work for dimensions with primitive $3$ or $5$.

In order to accomplish our goal, we introduce new techniques for self-testing.
First of all, we use a different variant of the weighted CHSH inequality to enforce the eigenvalue of
some unknown operator which is decomposed into two binary observables, which is not done before.
Secondly, we give a new way to decompose unitaries of arbitrary order into binary observables which maintain
certain commutation relations. Such decomposition is different from what Slofstra used in his work \cite{slofstra2017}.
Intuitively, such decomposition can be seen as the inverse of the decomposition used in the proof of Jordan's lemma.
The third contribution is that we prove self-testing without using anti-commutation relations between Pauli operators, 
which is the core idea in all the previous self-testing results. 
Instead, we find a new pair of operators that can generate the ring of matrices over complex numbers.


\textbf{Structure of the paper}.
We start with notations and background information in \cref{sec:prelim}.
Since the game we designed consists of two sub-tests, we introduce one test
in \cref{sec:lsg} and one in \cref{sec:chsh}. Our main result is based the combination
of the sub-tests and presented in \cref{sec:main}. In \cref{sec:extend}, we show how to adapt
our nonlocal game to self-test maximally entangled state of different dimensions.

%========================================
\section{Preliminaries and notations}
\label{sec:prelim}
%========================================
We use $[n]$ to denote the set $\{0,1 \dots n-1\}$ and $[n]+1$ for the set $\{1,2 \dots n\}$.
%We denote the group commutator of $A$ and $B$, i.e. $ABA^{-1}B^{-1}$, by $[A,B]$.
The EPR pair of local dimension $d$ for $d \geq 2$ is the maximally entangled state and is denoted by
\begin{align}
\ket{\EPR{d}} = \frac{1}{\sqrt{d}} \sum_{i = 0}^{d-1} \ket{ii}.
\end{align}
The superscript $(d)$ stresses the local dimension and we follow this convention through this paper.

We self-test $\ket{\EPR{d}}$ by verify that Alice and Bob has operators 
which have operators of the similar form
\begin{align}
	X = \sum_{k=1}^{d-1} \omega_d^k\ketbra{k}{k} && U = \sum_{k=1}^{d-1} \ketbra{k/2}{k},
\end{align}
where $\omega_d = e^{i2\pi/d}$ is the primitive $d$th root of unity.
Through out this paper, any operation on the label of the eigenvector $k$ is taken modulo $d$,
unless otherwise specified, for example, the $i/2$ above.

\textbf{The weighted CHSH inequality \cite{acin2012}.}
The first building-block of our result is a robust self-testing result based on the weighted CHSH inequality.
The weighted CHSH inequality is given below 
\begin{align}
	\label{eq:chsh_op}
	\I_\alpha = \alpha\ip{A_0B_0}+\alpha\ip{A_0B_1} + \ip{A_1B_0} - \ip{A_1B_1}\leq 2\alpha,
\end{align}
where $A_x,B_y$ for $x,y = 0,1$ are Binary observables on Hilbert space $\calH_A$ and $\calH_B$ respectively.
If Alice and Bob share product state $\ket{\phi} = \ket{\phi_A} \x \ket{\phi_B}$, they cannot violate
the weighted CHSH inequality.
However, If they share entangled state $\ket{\psi}$, the maximal violation is 
\begin{align}
\label{eq:chsh_max}
 \I_\alpha \leq 2\sqrt{1+\alpha^2}.
\end{align}
\begin{definition}[Ideal strategy for $\I_\alpha$]
	\label{def:ideal}
	Define $\mu = \arctan(1/\alpha)$.
	The ideal strategy for weighted CHSH with parameter $\alpha$ (i.e. achieving maximal violation in \cref{eq:chsh_max})
	consists of the joint state $\ket{\EPR{2}}$ and observables $A_0 = \pauliz$, $A_1 = \paulix$,
	$B_0 = \cos(\mu) \pauliz+ \sin(\mu) \paulix$ and $B_1 = \cos(\mu) \pauliz - \sin(\mu) \paulix$.
\end{definition}
We take an approach introduced in Ref.~\cite{bamps2015} and prove the following robust self-testing result.
\begin{theorem}
\label{thm:selftest}
	Suppose the quantum strategy $(\ket{\tpsi}, \{\tilde{A}_x\}_{x \in [2]}, \{\tilde{B}_y\}_{y \in [2]})$ achieves the violation
	at least $2\sqrt{1+\alpha^2} - \epsilon$
	for some $\epsilon$, then
	there exists a local isometry $\Phi = \Phi_A \x \Phi_B$ and an auxiliary state $\ket{aux}$  such that
	\begin{align}
		\| \Phi( \tilde{A}_x \x \tilde{B}_y \ket{\psi}) -\ket{aux} \x (A_x \x B_y) \ket{\EPR{2}}  \| = O(\sqrt{\epsilon})
	\end{align}
	for $x,y \in \{-1, 0, 1\}$ where the subscript $-1$ refers to the identity operator and $A_x, B_y$ are 
	defined in \cref{def:ideal}.
\end{theorem}
We defer the proof of \cref{thm:selftest} till Appendix~\ref{sec:selftest}.
After proving the robust self-testing result, we take one step further and observe an interesting property of the 
observable $\tilde{B}_0\tilde{B}_1$.
\begin{proposition}
\label{prop:2d-subspace}
	Suppose a quantum strategy $(\ket{\tpsi}, \{\tilde{A}_x\}_{x \in [2]}, \{\tilde{B}_y\}_{y \in [2]})$ achieves the maximal 
	violation of  $\I_{\cot(-\pi/2d)}$, then there exists a $2$-dimensional Hilbert space which is spanned by eigenvectors of 
	$\tilde{B}_0\tilde{B}_1$ of eigenvalue $\omega_d$ and $\omega_d^{-1}$.
\end{proposition}
\begin{proof}[Proof of \cref{prop:2d-subspace}]
	By \cref{thm:selftest}, the condition that the strategy $(\ket{\tpsi}, \{\tilde{A}_x\}_{x \in [2]}, \{\tilde{B}_y\}_{y \in [2]})$ achieves the  
	maximal value of $\I_{\cot(-\pi/2d)} $ implies that there exist states $\ket{u_0}, \ket{u_1} \in \supp(\Tr_A(\ketbra{\tpsi}{\tpsi}))$ such that 
	\begin{align*}
	(\tilde{B}_0 + \tilde{B}_1) \ket{u_0} = 2\cos(-\pi/2d) \ket{u_0} 
	&&(\tilde{B}_0 + \tilde{B}_1) \ket{u_1} = -2\cos(-\pi/2d) \ket{u_1}\\
	(\tilde{B}_0 - \tilde{B}_1) \ket{u_0} = 2\sin(-\pi/2d) \ket{u_1} 
	&&(\tilde{B}_0 - \tilde{B}_1) \ket{u_1} = 2\sin(-\pi/2d) \ket{u_0}.
	\end{align*}
	It is straightforward to calculate that 
	\begin{align*}
		\tilde{B}_0\tilde{B}_1 \ket{u_0} = \cos(\pi/d) \ket{u_0} -\sin(\pi/d) \ket{u_1},&&
		\tilde{B}_0\tilde{B}_1\ket{u_1} = \sin(\pi/d)\ket{u_0} + \cos(\pi/d) \ket{u_1},
	\end{align*}
	so we can conclude that 
	\begin{align*}
		&\tilde{B}_0\tilde{B}_1(\ket{u_0} + i\ket{u_1}) = e^{i \frac{\pi}{d}} (\ket{u_0} + i\ket{u_1}), &&
		&\tilde{B}_0\tilde{B}_1(\ket{u_0} - i\ket{u_1}) = e^{-i \frac{\pi}{d}} (\ket{u_0} - i\ket{u_1}).
	\end{align*}
	The actual eigenbasis we use is 
	\begin{align}
		\ket{x_1} = \frac{-1}{\sqrt{2}}(\ket{u_0} + i\ket{u_1}), &&
		\ket{x_{d-1}} = \frac{-e^{i\pi/2d}}{\sqrt{2}}(\ket{u_0} - i\ket{u_1})
	\end{align}
	so that 
	\begin{align}
		&B_0 = \omega_d\ketbra{x_1}{x_{d-1}} + \omega_d^{-1} \ketbra{x_{d-1}}{x_1},\\
		&B_1 =\ketbra{x_1}{x_{d-1}}  + \ketbra{x_{d-1}}{x_1},
	\end{align}
	which is important for the whole embedding argument to work in \cref{sec:comp}.
\end{proof}

\textbf{Nonlocal games}. The two players of a nonlocal game are Alice and Bob. Each of them is requested
to give an answer for a randomly chosen question. We denote Alice's question set by $\calX$ and answer set by $\calA$. Similarly,
Bob's question set is denoted by $\calY$ and his answer set is denoted by $\calB$. The nonlocal game also
comes with two functions: $\pi: \calX \times \calY \rightarrow [0,1]$, which is the probability distribution over the questions,
and $V: \calA \times \calB \times \calX \times \calY \rightarrow \R$, which is the scoring function. Such games are nonlocal
because Alice and Bob cannot communicate after getting their questions but they may share some strategy before 
the start of the game. Note that in the literature, the typical scoring function of a nonlocal game maps the input-output
pair to $\{0.1\}$ which corresponds to losing and winning. Allowing the score to be any real number is the key ingredient 
to our new nonlocal game. 

The quantum strategy of a game $G$ consists of projective measurements $\{\{A_x^a\}_a\}$ on Alice's side and 
$\{\{B_y^b\}_b\}_y$ on Bob's side, and a shared state $\ket{\psi}$. Then the behaviour of Alice and Bob is described 
by the conditional probability
\begin{align}
	\pr{ab}{xy} = \bra{\psi} A_x^a \x B_y^b \ket{\psi} \text{ for all } (a,b,x,y) \in \calA \times \calB \times \calX \times \calY,
\end{align}
where $(A_x^a)^2 = A_x^a = (A_x^a)^\dagger$ and $(B_y^b)^2 = B_y^b = (B_y^b)^\dagger$.
The \emph{value} of a strategy is given by
\begin{align}
	\omega(G,p)  = \sum_{a,b,x,y} \pi(x,y) \pr{ab}{xy} V(a,b,x,y).
\end{align} 

The main contribution of our work is the construction of a nonlocal game $\G$ with constant-sized $\calX$,$\calY$,$\calA$ and $\calB$
such that its optimal correlation can self-test $\ket{\EPR{d}}$ for arbitrary $d \in D$. 
The nonlocal game is a linear system game with modifications and strict enforcements. 
We introduce the definition of self-testing first and then the definition of linear system game, 
following definitions given in Ref.~\cite{coladan2017, slofstra2017}.
\begin{definition}[Self-testing]
	We say that a nonlocal game self-tests a quantum state $\ket{\Psi}$ if 
	any quantum strategy $S$ that achieves the optimal quantum value uses a shared state equivalent up to local isometry to $\ket{\Psi}$.
\end{definition}
\begin{definition}[Linear system game]
 Let $Hx = b$ be an $m \times n$ linear system over $\Z_2$. The associated linear system game involves two
 players Alice and Bob, where Alice is given an equation number $1 \leq x \leq m$ and replies with $a \in \Z_2^{\times n}$,
 and Bob is given a variable number $y$ and replies with an assignment $b \in \Z_2$. The winning condition is 
 \begin{align*}
 	a(y) &= b && \text{(Consistency)} \\
	\sum_{y = 1}^n H_{xy} a(y) &\equiv b(x) \pmod 2. &&\text{(Constraint satisfaction)}
 \end{align*}
\end{definition}
The scoring function of linear system games always maps an input-output pair to $\{0,1\}$, 
so later when we say a quantum strategy wins a linear system game perfectly, we mean that 
$V(a,b,x,y) =0$ implies that $\pr{ab}{xy} = 0$.

%The main tool to understand linear system game is through its solution group over $\Z_d$.
%\begin{definition}[Solution group over $\Z_d$ \cite{coladan2017}]
%	For the linear system game associated with $Ax = b$ over $\Z_d$, its solution group $\Gamma(A,b,\Z_d)$ has 
%	one generator for each variable and one relation for each equation and relations enforcing that the variables in the same
%	equation commutes. The set of local commutativity relations is denoted by $R_c$ and defined by
%	\begin{align}
%		R_c := \set{ [x_i, x_j] |  A_{li} \neq 0 \neq A_{lj} \text{ for some } 1 \leq l \leq m}.
%	\end{align}
%	The set of constraint satisfaction relations is denoted by $R_{eq}$ and defined by.
%	\begin{align}
%		R_{eq} := \set{ \J^{-b(l)} \Pi_{i=1}^n  x_i^{A_{li}} | 1\leq l\leq m}
%	\end{align}
%	Then the solution group has presentation 
%	\begin{align}
%	\Gamma(A,b,\Z_d) := \ip{ \{x_i\}_{i=1}^n \cup \{\J\} : R_c \cup R_{eq} \cup \{ (x_i)^d, \J^d| 1 \leq i \leq n\}}. 
%	\end{align}
%\end{definition}
%Note that the relations $(x_i)^d$ and $\J^d$ ensure that we have solutions over $\Z_d$.
%Then the operator solution of $\Gamma(A,b,\Z_d)$ is given below.
%\begin{definition}[Operator solution]
%	An operator solution for the linear system game associated with $Ax =b$ over $\Z_d$ is a unitary representation
%	$\tau$ of $\Gamma(A,b,\Z_d)$ such that $\tau(\J) = \omega_d\1$. A conjugate operator solution is a unitary 
%	representation mapping $\J$ to $\overline{\omega_d}\1$.
%\end{definition}
%What has been established in Ref.\cite{cleve2017,coladan2017} is that we can construct a perfect strategy of a
%linear system game from its operator solution and vice versa.


%=======================================
\section{The linear system game}
\label{sec:lsg}
%=======================================
The goal of this section and the following two sections is to 
present a correlation that can self-test $\ket{\EPR{d-1}}$, 
where $d$ is prime and has primitive root $2$. 
If $d$ has primitive root $2$,
then $2$ is the generator of the multiplicative group of integers modulo $d$, $\Z_d^\times$.
When $d$ is prime, $\Z_d^\times = \{1,2\dots d-1\}$.
The correlation we constructed is the optimal correlation for a game $\G$, which is the combination of two tests.
In this section, we introduce the linear system game which tests a relation that should be satisfied by 
Alice and Bob's observables.

Slofstra's seminal work~\cite{slofstra2017} draws our attention to the relation $xyx^{-1} = y^2$, which
is included in the group $K$, which has presentation
\begin{align*}
	K = \langle a,b,c,x,y: a^2=b^2=c^2 = e, c=ab, yay^{-1} = a, yby^{-1}= c, xyx^{-1} = y^2\rangle.
\end{align*}
The main component of $\G$ is a linear system game $\LS$, whose solution group is the embedding of the 
following group
\begin{align}
	\Pg= \langle u, x : uxu^{-1} = x^2 \rangle.
\end{align}

\begin{proposition}
	\label{prop:embed}
	The group $\Pg$ can be embedded into a linear system game, $\LS$, over $\Z_2$,
	with $118$ variables and $91$ equations, where each equation involves $3$ variables.
\end{proposition}
The process of embedding $\Pg$
into the solution group of $\LS$ follows the recipe given in the proofs of
Proposition~$4.8$, Lemma~$4.4$ and Proposition $4.2$ of Slofstra's seminal work \cite{slofstra2017}.
The order of applying the results is the reverse of the order they are presented in Ref.~\cite{slofstra2017}..
\begin{proof}
The first step is to embed $\Pg$ into a linear-plus-conjugacy group $\Pg''$ where
all the generators are of order $2$, following the instructions given in the proof of  
Proposition~$4.8$ of the Ref.~\cite{slofstra2017}.
We first introduce $x_1,x_2$ such that $x_1^2=x_2^2=e$, $x=x_1x_2$ and $u x_2 u^{-1}=x_2$.
We also introduce $x' = ux_1u^{-1}$, 
then we embed $\Pg$ in $\Pg'$ as
\begin{align}
	\Pg' = \langle x_1,x_2,x',u: x_1^2=x_2^2=x'^2=e,
	x_1x_2x_1=x', ux_1u^{-1}=x', ux_2u^{-1} =x_2 \rangle.
\end{align}
Next, we introduce $u_1,u_2$ such that $u_1^2=u_2^2=e$ and $u=u_1u_2$, 
and also $u_3.u_4$ of order $2$ such that the conjugacy relations involving them are 
\begin{align*}
u_2x_2u_2 = u_3, && u_1u_3u_1 = x_2\\
u_2x_1u_2 = u_4, && u_1u_4u_1 = x',
\end{align*}
then
$\Pg'$ is further embedded in $\Pg''$ as
\begin{equation}
\begin{aligned}
	\Pg'' =  \langle &x_1,x_2,x',u_1,u_2,u_3,u_4: \text{all elements are of order $2$},\notag\\
	 &u_2x_2u_2 = u_3, u_1u_3u_1 = x_2,
u_2x_1u_2 = u_4, u_1u_4u_1 = x',x_1x_2x_1=x' \rangle.
\end{aligned}
\end{equation}	
To represent the conjugacy relations by a tuple, we relabel the some of the generators as
\begin{align*}
	u_1 = x_3, && u_2 = x_4, && x' = x_5, && u_3 = x_6, && u_4 = x_7,
\end{align*}
so that a simpler form of the group $\Pg''$ is 
\begin{equation}
\begin{aligned}
	\Pg'' = \langle \{x_i\}_{i=1}^7 : x_i^2 = e \; \forall i, 
	x_1x_2x_1=x_5, x_4x_2x_4 = x_6,
	 x_4x_1x_4=x_7, x_3x_2x_3 = x_6, x_3x_5x_3=x_7\rangle
\end{aligned}
\end{equation}
We collect the subscript of $x_i$'s in the conjugacy relations and define
\begin{align}
	C= \{ (1,2,5), (3,5,7), (4,1,7), (4,2,6), (3,2,6)\},
\end{align}
so that $(i,j,k) \in C$ if and only if $x_ix_jx_i = x_k$ in $\Pg''$.

Next we embed $\Pg''$ into $\Pg_{NLC}$ to make $x_j$ and $x_k$ commute if $(i,j,k) \in C$,
where $\Pg_{NLS}$ is a nice linear-plus-conjugacy group.
This steps are given in the proof of Lemma~$4.4$ of the Ref.~\cite{slofstra2017}.
The group $\Pg_{NLS}$ is defined as 
\begin{equation}
\begin{split}
\Pg_{NLS} = \langle \{x_i, w_i, y_i, j_i\}_{i=1}^{7},f : &f^2 = x_i^2 =w_i^2=y_i^2=j_i^2= e, \\
	&x_i = y_iz_i = fw_i,\; fy_if =z_i \quad\text{where } 1 \leq i \leq 7 \\
	&y_j z_k = z_ky_j,\; w_iy_jw_i = z_k \quad\text{for all } (i,j,k)\in C \rangle.
\end{split}
\end{equation}
To summarize the linear relations, we first introduce generator $g_{jk} = y_j z_k$ for all $(i,j,k) \in C$
such that $g_{jk}^2 = e$. Now the group has $34$ generator.
The $19$ linear relations are
\begin{align*}
	&x_i y_i z_i = e \quad \text{for} \quad i =1\dots 7 \\
	&x_i f w_i = e  \quad \text{for} \quad i =1\dots 7\\
	&g_{jk}y_jz_k =e \quad \text{for all}\quad (i,j,k) \in C.
\end{align*}
The $12$ conjugacy relations are 
\begin{align*}
	&fy_if = z_i\quad \text{for} \quad i =1\dots 7\\
	&w_iy_j w_i = z_k \quad\text{for all } (i,j,k)\in C.
\end{align*}
We can standardise the generators by relabeling the generators all as $x_i$.
There are $29$ generators in $\Pg_{NLS}$.

The last step is given in the proof of Proposition $4.2$ of Ref.~\cite{slofstra2017}, which
embeds all the conjugacy relations in linear relations.
This is done by introduce $7$ new variables and $6$ linear relations for each
conjugacy relations. For example, If $I = (i,j,k)$ is a conjugacy relation, then we introduce
generators $y_{Ii}$ for $i=1 \dots 6$. Note that $y_{Ii}$ shall not be confused with the $y_i$'s
in the previous step. The new linear relations are
\begin{align*}
	x_i y_{I1} y_{I2} = e && x_j y_{I2} y_{I3} = e && y_{I3}y_{I4}y_{I5} = e\\
	x_i y_{I5} y_{I6} = e && x_k y_{I6} y_{I7} = e && y_{I1}y_{I4}y_{I7} = e.
\end{align*}
In the end, the solution group of $\LS$, $\Pg_{\LS}$,  
has $118$ variables and $91$ equations.
\end{proof}
In the later parts of the paper, we use $n = 118$ and $m = 91$ to refer to the size of $\LS$.
The key feature of this embedding is that $\LS$ has constant-sized input-output alphabet
even if the generators $u$ and $x$ can have arbitrary order. 

We denote the special operators in the operator solution of $\LS$ that correpsond to generators 
$u = x_3x_4$ and $x = x_1x_2$ by $U$ and $S$.
The special property of $U$ and $S$ is summarized in the following lemma. 
\begin{lemma}
	\label{lm:ux_independ}
	Suppose there exist unitaries $U$ and $X$ which have the following forms
	\begin{align}
		X = \sum_{i=1}^{d-1} \omega_d^i \ketbra{i}{i} && U = \sum_{i=1}^{d-1}\ketbra{i/2}{i},
	\end{align}
	then the set $\{U^k X^l\}$ for $k=0,1\dots d-2$ and $l = 1,2\dots d-1$ forms a basis of 
	the ring of $(d-1)\times (d-1)$ matrices over $\C$.
\end{lemma}
Note that the unitaries $U$ and $X$ defined in the lemma above satisfy the condition $UXU^\dagger = X^2$.
In the self-test proof, this lemma will be a critical step.
\begin{proof}
We are going to show the $(d-1)^2$ matrices from the set $\{U^k X^l\}_{k \in[d-1], l \in [d-1]+1}$ are linearly independent.
Suppose there exists a set of complex numbers $\{ x_{k,l} \}_{k \in[d-1], l \in [d-1]+1}$
such that 
\begin{align}
	M = \sum_{k=0}^{d-2} \sum_{l=1}^{d-1} x_{k,l} U^k X^l = 0. 
\end{align}
We further assume that there exists a set of integers $\{ k_i \}_{i=1}^{d-1}$ such that $2^{k_i} \equiv i \pmod{d}$.
The fact that $2$ is a primitive root of $d$ guarantees that $k_i$'s are distinct.
Then we can group $\{x_{k,l}\}$ into vectors: $\ket{x_{k_1}}, \ket{x_{k_2}} \dots \ket{x_{k_{d-1}}}$,
where $\ket{x_{k_i}}= (x_{k_i, 1}, x_{k_i, 2} \dots x_{k_i, d-1})^\intercal$.
Our goal is equivalent to proving that $\ket{x_{k_i}} = 0$ for all $i$.

We start with proving that $\ket{x_{k_1}} = 0$.
Proving $\ket{x_{k_i}} = 0$ for other $i$ follows a similar argument, so we briefly
discuss about it in the end.
The entry $\bra{1}M\ket{1}$ can be expressed as  
\begin{align}
	\bra{1}M\ket{1} = \sum_{k=0}^{d-2}\sum_{l = 1}^{d-1}\sum_{i=1}^{d-1} x_{k, l}\omega_d^{il}\braket{1}{i/2^k}\braket{i}{1}.
\end{align}
For the term $\braket{1}{i/2^k}\braket{i}{1} \neq 0$ we must have $i = 1$ and $2^k \equiv 1 \pmod{d}$, or equivalently,
$k = k_1$. Hence, we can conclude that 
\begin{align}
	\bra{1}M\ket{1} = \sum_{l = 1}^{d-1} x_{k_1,l}\omega_d^l = 0. 
\end{align}
Similarly we can determine that for all $j = 1,2\dots d-1$,
\begin{align}
	\bra{j}M\ket{j} 
	=  \sum_{k=0}^{d-2}\sum_{l = 1}^{d-1}\sum_{i=1}^{d-1} x_{k, l}\omega_d^{il}\braket{j}{i/2^k}\braket{i}{j} 
	= \sum_{l = 1}^{d-1}x_{k_1,l}\omega_d^{jl} = 0.
\end{align}
Hence we get $d-1$ equations with $d-1$ variables, and the linear system is
\begin{align}
	W \ket{x_{k_1}} = 0,
\end{align}
where $W_{m,n} = \omega_d^{mn}$. Then we define
\begin{align}
	\tW = 
	\begin{pmatrix}
	1 & 1 \\
	1 & W
	\end{pmatrix}.
\end{align}
First observe that $\tW$ is a Vandermonde matrix, hence it is non-singular.
Next, we define $\ket{\tx_{k_1}} = (0, x_{k_1,1}, \dots x_{k_1,d-1})^\intercal$ 
and prove that it satisfies the condition
that 
\begin{align}
	\tW \ket{\tx_{k_1}} = 0,
\end{align}
which involves $d$ equations. The last $d-1$ equations are given by the assumption and $M$.
We only need to prove that $\sum_{l=1}^d x_{k_1, l} = 0$, which is required by the first row of $\tW$.
It can be proved by summing the known $d-1$ equations as follows
\begin{align}
	0=\sum_{j = 1}^{d-1} \bra{j}M\ket{j}  
	=  \sum_{j=1}^{d-1}\sum_{l = 1}^{d-1}x_{k_1,l}\omega_d^{jl}
	=\sum_{l = 1}^{d-1}x_{k_1,l} (\sum_{j=1}^{d-1} \omega_d^{jl})
	= \sum_{l = 1}^{d-1}- x_{k_1,l}
\end{align}
where we have used the fact that $\sum_{j=1}^{d-1} \omega_d^{jl} =-1$ for all $l = 1,2\dots d-1$.
Since $\tW$ is non-singluar, we know $\ket{\tx_{k_1}} = 0$ which implies that $\ket{x_{k_1}} = 0$.

For $\ket{x_{k_a}}$, we look at entries $\{\bra{j}M\ket{aj}\}_{j=1}^{d-1}$ for $a = 2 \dots d-1$ and get equations
of the form
\begin{align}
	0 = \bra{j}M\ket{aj} = \sum_{l=1}^{d-1} x_{k_a, l} \omega_d^{ajl} 
\end{align}
The corresponding coefficient matrix has value $\omega_d^{amn}$ at coordinate $(m,n)$,
so it is also a submatrix of a Vandermonde matrix. Similar argument gives us that $\ket{x_{k_a}} = 0$.

To summarize, we have proven that $x_{k,l} = 0$ for all $k$ and $l$, which implies that the elements of the set
$\{ U^k X^l \}$ are linearly independent and forms a basis for the ring of all the $(d-1)\times(d-1)$ matrices over $\C$.
\end{proof}
%\begin{align}
%	\Pg_d = \langle x, z, \J : x^d = z^d = \J^d = e, zxz^{-1}x^{-1} = \J, x\J x^{-1}\J^{-1}= z\J z^{-1}\J^{-1} = e\rangle. 
%\end{align}
%The goal of our modification is to construct $\Pg_{\LS}$ which has implicit $d$-dependence.
%We first introduce new generators $u_x$ and $u_z$ to $\Pg_d$, and replace the relation $x^d = z^d = e$ 
%by the following relations
%\begin{align}
%\label{eq:sim}
%	u_x x u_x = x^2, \quad
%	u_z z u_z = z^2.
%\end{align}
%Consequently, the $d$ dependency of $\G$ comes from constraints imposed by other components of the game,
%\footnote{Figuring out what $a$ is will take us one step closer to resolving Artin's Conjecture\cite{murty1988}.}
%but the alphabet sizes are determined by $\Pg_{\LS}$ and we will see why they are constant.
%Next, we drop the relation $\J^d = e$ and make the value of $\J$ determined by $x$ and $z$ in the 
%relation $xzx^{-1}z^{-1} = \J$.
%
%It can be easily checked that $\paulix{d}$ and $\pauliz{d}$ can be extended to a representation of $\Pg_{\LS}$ 
%where  $\paulix{d}$ and $\pauliz{d}$ are defined by
%\begin{align}
%	\paulix{d} = \sum_{i=0}^{d-1} \ketbra{i+1 \pmod{d} }{i}  \quad \quad
%	\pauliz{d} = \sum_{i=0}^{d-1} \omega_d^i \ketbra{i}{i}.
%\end{align}
%Moreover, if $U_x\paulix{d}U_x^\dagger = (\paulix{d})^2$ and $U_z\pauliz{d}U_z^\dagger = (\pauliz{d})^2$,
%we can verify that 
%\begin{align}
%	U_xU_z = \1 = U_zU_x,
%\end{align}
%Hence, we do not need both $u_x$ and $u_z$ as generators and we just need one of them.
%In the end, we define $\Pg_{\LS}$ by
%\begin{equation}
%\begin{aligned}
%	\Pg_{\LS} =  \langle x, z, u, \J :  &zxz^{-1}x^{-1} = \J, [x,\J]=[z,\J]=[u,\J] = e, \\
%	&uxu^{-1} = x^2, u^{-1}zu = z^2 \rangle. 
%\end{aligned}
%\end{equation}
%This group will be embedded in a solution group, $\Gamma(\LS)$, following Slofstra's embedding techniques.
%The corresponding game $\LS$ has $n = 2351$ variables and $m= 1916$ equations.

%======================================
\section{The extended weighted CHSH game}
\label{sec:chsh}
%======================================
The extended weighted CHSH game is added to make sure that the operator $X$ extracted from 
Alice and Bob's operator solution have eigenvalues $\omega_d$ and $\omega_d^{d-1}$.
In \cref{sec:main}, we will reason why showing these two eigenvalues is enough to guarantee that 
$U$ and $X$ has the eigen-structure required by \cref{lm:ux_independ}.
We denote this game that enforces the eigenvalues of the observable $U$ by
$\CHSH_U$, where the superscript $d$ emphasizes that the scoring rules of this game depend on $d$.


In this game, Alice and Bob each gets a question $x$,$y \in \{ 0, 1, \ast\}$ and 
they answer with $a,b \in \{0,1,\diamond,\perp\}$. 
Our self-test result follows the optimal behaviour in this game,
Before presenting the optimal correlation, we give intuitions about how the players should behave.
\begin{itemize}
	\item \textbf{Case 1:} when $x = y = \ast$, Alice and Bob should answer with $a, b \in \{\diamond, \perp\}$ and 
	their answer should agree;
	\item \textbf{Case 2a:} when $x,y \in \{0,1\}$ and if they answer with $a,b \in \{0,1\}$, then
	their answers are scored according to $I_{\cot(-\pi/2d)}$;
	\item \textbf{Case 2b:} when $x,y \in \{0,1\}$ and if Alice answers with $\perp$, then all Bob's answers are irrelevant;
	\item \textbf{Case 3:} when $x \in {0,1}, y = \ast$, if Bob answers $\diamond$, 
	Alice should answer with $\{0.1\}$ but not $\perp$,
	if Bob answers $\perp$, Alice should answer $\perp$ too.
\end{itemize}
\textbf{The ideal strategy and ideal correlation}. Alice and Bob share the state $\ket{\psi} =\frac{1}{\sqrt{d-1}} \sum_{i=1}^{d-1} \ket{u_i}\ket{u_i}$.
We define two subspaces $V = \spn\{\ket{u_1}, \ket{u_{d-1}}\}$ and $V^\perp = \C^d \setminus\spn\{\ket{u_1}, \ket{u_{d-1}}\}$ and
define $\Pi_V$ and $\Pi_{V}^\perp$ to be the corresponding projectors. Note that $V$ is the subspace on which they should
maximize $\langle I_{\cot(-\pi/2d)} \rangle$.

For completeness, we show the ideal correlation in the following three charts and then give the projectors. 
Note that we don't explicitly calculate the conditional probabilities of the form $\pr{\perp 0}{xy}$ for all possible $x$,$y$ 
because they are irrelevant.
\begin{table}[H]
\begin{center}
\begin{tabular}{|c|c||c|c|}
\hline
\multicolumn{2}{|c|}{} &
\multicolumn{2}{|c|}{$x=\ast$}\\
\cline{3-4}
\multicolumn{2}{|c|}{} &$a = \diamond$ & $a = \perp$ \\
\hline
\hline
\multirow{2}{*}{$y = \ast$} & $b=\diamond$ & 2/(d-1) & 0 \\
\cline{2-4}
&$b=\perp$ & 0 & (d-3)/(d-1) \\
\hline
\end{tabular}
\caption{Alice and Bob's behaviour when $x=y=\ast$.}
\end{center}
\end{table}

\begin{table}[H]
\begin{center}
\begin{tabular}{|c|c||c|c|c|c|c|c|}
\hline
\multicolumn{2}{|c|}{} &
\multicolumn{3}{|c|}{$x=0$}&
\multicolumn{3}{|c|}{$x=1$} \\
\cline{3-8}
\multicolumn{2}{|c|}{} &
$a = 0$ & $a=1$ & $a=\perp$ &
$a = 0$ & $a=1$ & $a=\perp$\\
\hline
\hline
\multirow{2}{*}{$y = 0$} & $b=0$ & $\frac{\cos^2(\pi/4d)}{d-1}$ & $\frac{\sin^2(\pi/4d)}{d-1}$ & \small $\pr{\perp0}{00}$ 
& $\frac{1+\sin(\pi/2d)}{2(d-1)}$ & $\frac{1-\sin(\pi/2d)}{2(d-1)}$ & \small  $\pr{\perp0}{10}$ \\
\cline{2-8}
&$b=1$ & $\frac{\sin^2(\pi/4d)}{d-1}$ & $\frac{\cos^2(\pi/4d)}{d-1}$ & $\frac{d-3}{d-1}-\pr{\perp0}{00}$ 
&  $\frac{1-\sin(\pi/2d)}{2(d-1)}$ & $\frac{1+\sin(\pi/2d)}{2(d-1)}$ & \small $\frac{d-3}{d-1} - \pr{\perp0}{10}$  \\
\hline
\multirow{2}{*}{$y = 1$} & $b=0$ & $\frac{\cos^2(\pi/4d)}{d-1}$ & $\frac{\sin^2(\pi/4d)}{d-1}$ & \small $\pr{\perp0}{01}$ & 
$ \frac{1-\sin(\pi/2d)}{2(d-1)}$ & $ \frac{1+\sin(\pi/2d)}{2(d-1)}$ & \small $\pr{\perp 0}{11}$  \\
\cline{2-8}
&$b=1$ & $\frac{\sin^2(\pi/4d)}{d-1}$ & $\frac{\cos^2(\pi/4d)}{d-1}$ & \small $\frac{d-3}{d-1}-\pr{\perp0}{01}$ &  
$ \frac{1+\sin(\pi/2d)}{2(d-1)}$ & $ \frac{1-\sin(\pi/2d)}{2(d-1)}$ & \small $\frac{d-3}{d-1}- \pr{\perp 0}{11}$ \\
\hline
\end{tabular}
\end{center}
\caption{Alice and Bob's behaviour when $x,y \in [2]$.}
\end{table}

\begin{table}[H]
\begin{center}
\begin{tabular}{|c|c||c|c|c|c|c|c|}
\hline
\multicolumn{2}{|c|}{} &
\multicolumn{3}{|c|}{$x=0$}&
\multicolumn{3}{|c|}{$x=1$} \\
\cline{3-8}
\multicolumn{2}{|c|}{} &
$a = 0$ & $a=1$ & $a=\perp$ &
$a = 0$ & $a=1$ & $a=\perp$\\
\hline
\hline
\multirow{2}{*}{$y = \ast$} & $b=\diamond$ & $1/(d-1)$ & $1/(d-1)$ & 0 
& $1/(d-1)$ & $1/(d-1)$ & 0 \\
\cline{2-8}
&$b=\perp$ & 0 & 0 & $\frac{d-3}{d-1}$ 
&  0 & 0 & \small $\frac{d-3}{d-1} $  \\
\hline
\end{tabular}
\end{center}
\caption{Alice and Bob's behaviour when $x\in [2]$ and $y = \ast$.}
\end{table}
Alice's projectors are 
\begin{align*}
	&A_\ast^\diamond = \Pi_V, A_\ast^\perp = \Pi_V^\perp \\
	&A_0^0 = \ketbra{u_1}{u_1}, A_0^1 = \ketbra{u_{d-1}}{u_{d-1}}, A_0^\perp = \Pi_V^\perp\\
	&A_1^0 = \frac{1}{2}(\ket{u_1}+\ket{u_{d-1}})(\bra{u_1}+\bra{u_{d-1}}), 
	A_0^1 = \frac{1}{2}(\ket{u_1}-\ket{u_{d-1}})(\bra{u_1}-\bra{u_{d-1}}),A_0^\perp = \Pi_V^\perp.
\end{align*}
Bob's projectors are 
\begin{align*}
	&B_\ast^\diamond = \Pi_V, B_\ast^\perp = \Pi_V^\perp \\
	&B_0^0|_V = \left( \cos(\frac{\pi}{4d})\ket{u_1} + \sin(\frac{\pi}{4d})\ket{u_{d-1}}\right)
	\left( \cos(\frac{\pi}{4d})\bra{u_1} + \sin(\frac{\pi}{4d})\bra{u_{d-1}}\right)\\
	&B_0^1|_V = \left( \sin(\frac{\pi}{4d})\ket{u_1} - \cos(\frac{\pi}{4d})\ket{u_{d-1}}\right)
	\left( \sin(\frac{\pi}{4d})\bra{u_1} - \cos(\frac{\pi}{4d})\bra{u_{d-1}}\right)\\
	&B_1^0|_V = \left( \cos(\frac{\pi}{4d})\ket{u_1} - \sin(\frac{\pi}{4d})\ket{u_{d-1}}\right)
	\left( \cos(\frac{\pi}{4d})\bra{u_1} - \sin(\frac{\pi}{4d})\bra{u_{d-1}}\right)\\
	&B_0^1|_V = \left( \sin(\frac{\pi}{4d})\ket{u_1} + \cos(\frac{\pi}{4d})\ket{u_{d-1}}\right)
	\left( \sin(\frac{\pi}{4d})\bra{u_1} + \cos(\frac{\pi}{4d})\bra{u_{d-1}}\right).
\end{align*}
About Bob's projectors for input $y \in [2]$, we are only interested in their actions when restricted 
to the subspace $V$. Their actions on the subspace $V^\perp$ is irrelevant in this game.

The self-testing property of this correlation is summarized in the following lemma.
\begin{lemma}
	\label{lm:chsh_comp}
	Suppose a quantum strategy $\left(\{\{\tA_x^a\}_a\}_x, \{\{\tB_y^b\}_b\}_y, \ket{\tpsi}\right)$
	achieves the optimal correlation of $\CHSH$, 
	and let $\ket{\tpsi'} = \tA_\ast^\diamond \x \tB_\ast^\diamond \ket{\tpsi}/\norm{\tA_\ast^\diamond \x \tB_\ast^\diamond \ket{\tpsi}}^2$,
	then there exists unitaries $U_A$ and $U_B$ 
	and a quantum state $\ket{junk}$
	such that 
	\begin{align*}
		&U_A\x U_B \left( \ket{\tpsi'}\right) = \ket{\EPR{2}}\x \ket{junk} \\
		&U_A\x U_B \left[\left(\1 \x  \frac{\tB_0 + \tB_1}{2\cos(\pi/2d)}\right)\ket{\tpsi'}\right]
		=[(\1 \x \pauliz) \ket{\EPR{2}}] \x \ket{junk} \\
		&U_A\x U_B \left[\left(\1 \x  \frac{\tB_0 - \tB_1}{-2\sin(\pi/2d)}\right)\ket{\tpsi'} \right]
		=[(\1 \x \paulix) \ket{\EPR{2}}] \x \ket{junk} 
	\end{align*}
	where $\tB_0 = \tB_0^0 - \tB_0^1$ and $\tB_1 = \tB_1^0 - \tB_1^1$.
\end{lemma}
\begin{proof}
Note that $\tA_x^a \tB_y^b$ means $\tA_x^a \x\tB_y^b$ in the following proof.

From the marginal distribution $\pb{\diamond}{\ast} = \pa{0}{0}+\pa{1}{0} = 2/d-1$,
we know $\| \tB_\ast^\diamond \ket{\tpsi} \| = \| (\tA_0^0+\tA_0^1) \ket{\tpsi}\| = \sqrt{2/d-1}$.
Since $\pr{0\diamond}{0\ast} + \pr{1\diamond}{0\ast} = 2/d-1$, we find that 
\begin{align*}
	\frac{\bra{\tpsi}\tB_\ast^\diamond (\tA_0^0+\tA_0^1) \tB_\ast^\diamond\ket{\tpsi}}{ \| \tB_\ast^\diamond \ket{\tpsi} \|^2} = 1,
\end{align*}
which means that 
\begin{align}
	(\tA_0^0+\tA_0^1)\tB_\ast^\diamond \ket{\tpsi} = \tB_\ast^\diamond \ket{\tpsi}.
\end{align}
Using the commutation relation between $(\tA_0^0+\tA_0^1)$ and $\tB_\ast^\diamond$, we get
\begin{align*}
	\frac{\bra{\tpsi} (\tA_0^0+\tA_0^1) \tB_\ast^\diamond(\tA_0^0+\tA_0^1)\ket{\tpsi}}{ \| (\tA_0^0+\tA_0^1) \ket{\tpsi} \|^2} = 1,
\end{align*}
Similar argument gives us that 
\begin{align}
	\tB_\ast^\diamond (\tA_0^0+\tA_0^1) \ket{\tpsi} = (\tA_0^0+\tA_0^1)\ket{\tpsi}.
\end{align}
The two equations above can be chained by commutativity to reach the conclusion that 
\begin{align}
	(\tA_0^0+\tA_0^1)\ket{\tpsi} = \tB_\ast^\diamond \ket{\tpsi}.
\end{align}
Following the same line of argument, we can conclude that
\begin{align}
	\tB_\ast^\diamond \ket{\tpsi} = \tA_\ast^\diamond \ket{\tpsi} = (\tA_0^0+\tA_0^1)\ket{\tpsi} = (\tA_1^0+\tA_1^1)\ket{\tpsi}.
\end{align}
Looking at the marginal distribution when Alice and Bob output $\perp$, we conclude that 
\begin{align}
	\tB_\ast^\perp \ket{\tpsi} = \tA_\ast^\perp \ket{\tpsi} = \tA_0^\perp \ket{\tpsi} = \tA_1^\perp \ket{\tpsi},
\end{align}
with similar arguments.

Next we examine the CHSH-type correlation when $x,y \in [2]$,
\begin{align*}
	   \bra{\tpsi} \tA_0^0\tB_0^0 \ket{\tpsi} 
	= &\bra{\tpsi}(\tA_\ast^\diamond + \tA_\ast^\perp) \tA_0^0\tB_0^0 (\tA_\ast^\diamond + \tA_\ast^\perp)\ket{\tpsi} \\
	= & \bra{\tpsi}\tA_\ast^\diamond \tA_0^0\tB_0^0 \tA_\ast^\diamond\ket{\tpsi} + \bra{\tpsi}\tA_\ast^\diamond \tA_0^0\tB_0^0 \tA_\ast^\perp\ket{\tpsi} \\
	&+\bra{\tpsi}\tA_\ast^\perp \tA_0^0\tB_0^0 \tA_\ast^\diamond\ket{\tpsi} + \bra{\tpsi}\tA_\ast^\perp \tA_0^0\tB_0^0 \tA_\ast^\perp\ket{\tpsi}\\
	= & \bra{\tpsi}\tB_\ast^\diamond \tA_0^0\tB_0^0 \tB_\ast^\diamond\ket{\tpsi} + \bra{\tpsi}\tA_\ast^\diamond \tA_0^0\tB_0^0 \tA_0^\perp\ket{\tpsi} \\
	&+\bra{\tpsi}\tA_0^\perp \tA_0^0\tB_0^0 \tA_\ast^\diamond\ket{\tpsi} + \bra{\tpsi}\tA_0^\perp \tA_0^0\tB_0^0 \tA_0^\perp\ket{\tpsi}\\
	=&\bra{\tpsi}\tB_\ast^\diamond \tA_0^0\tB_0^0 \tB_\ast^\diamond\ket{\tpsi},
\end{align*}
where we use the facts that $\tB_\ast^\diamond \ket{\tpsi} = \tA_\ast^\diamond \ket{\tpsi}$, $\tA_\ast^\perp \ket{\tpsi} = \tA_0^\perp \ket{\tpsi}$ and that 
$\spn(\tA_0^0) \cap \spn(\tA_0^\perp) = \emptyset$. 
This means that if Alice and Bob share the state $\tB_\ast^\diamond \ket{\tpsi}/\|\tB_\ast^\diamond \ket{\tpsi}\|$ and apply 
$\tA_0^0\tB_0^0$, the conditional probability is
\begin{align}
	\frac{\bra{\tpsi}\tB_\ast^\diamond \tA_0^0\tB_0^0 \tB_\ast^\diamond\ket{\tpsi}}{\bra{\tpsi} \tB_\ast^\diamond \tB_\ast^\diamond \ket{\tpsi}} = \frac{\cos^2(\pi/4d)}{2}.
\end{align} 
We can re-normalize the other correlations of $a,b \in [2]$ when $x,y \in [2]$ similarly, and get a new set of correlations 
which achieves the maximal value of $\langle \1_{-\cot(\pi/2d)}\rangle$. 
The conclusion of \cref{lm:chsh_comp} follows the application of \cref{thm:selftest} on the state $\tA_\ast^\diamond\tB_\ast^\diamond \ket{\tpsi}/\|\tA_\ast^\diamond\tB_\ast^\diamond \ket{\tpsi}\|$ as $\tA_\ast^\diamond\tB_\ast^\diamond \ket{\tpsi} = \tB_\ast^\diamond \ket{\tpsi}$.
\end{proof}
Note that the combination of \cref{lm:chsh_comp} with \cref{prop:2d-subspace} gives us that 
$\tB_0\tB_1$ has eigenvalues $\omega_d$ and $\omega_d^{-1}$.



%=====================================
\section{Main result}
\label{sec:main}
%=====================================
In this section, we introduce the combined game $\G{d-1}$ and then
prove that it can self-test the state $\ket{\EPR{d-1}}$.

%-----------------------------------------------------------------
\subsection{The correlation for the game $\G$}
%-----------------------------------------------------------------
Assume the linear system game has $n=118$ variables and $m=91$ equations.
Alice receives $x \in \{1,\dots m+3 \}$ and Bob receives
$y \in \{1,\dots,n+1\}$. We follow the previous structure by first give intuition about how they should
behave in this game. The correlation can be easily extracted from the behaviour list below.
\begin{itemize}
	\item When $x \in \{1,\dots m\}$ and $y \in \{1, \dots n\}$, they should win the 
	linear system game $\LS$ perfectly;
	\item when $x \in \{m+1, m+2, m+3\}$ and $y \in \{1, 2, n+1\}$, they should follow the
	optimal correlation of the test $\CHSH_X$, where 
	\begin{align}
		&\ast_A = m+1, \quad 0_A = m+2,\quad 1_A = m+3,\\
		&\ast_B = n+1,\quad 0_B = 1, \quad 1_A = 2,
	\end{align}
	are the inputs for the game $\CHSH_X$
	(The intuition behind is that $B_1B_2 = X$.);
	\item otherwise, their behaviour is irrelavant.
\end{itemize}
Note that the dimension $d$ is defined in the rules of $\CHSH_X$.

\begin{proposition}
	The game $\G{d-1}$ can be won perfectly by a quantum strategy.
\end{proposition}
We prove this proposition by giving the optimal strategy.
\begin{proof}
We determine the operator solution first and then we determine the shared state.
Since Bob gets the same symbols $1$ and $2$ in different sub-tests, 
the observables $B_1$ and $B_2$ are extended from the formulation given in the proof
of \cref{prop:2d-subspace},
%\begin{align}
%	B_1 = \ketbra{x_0}{x_0} + 
%	\sum_{k=1}^{(d-1)/2}\left( e^{ik\pi/2d}\ketbra{x_k}{x_{d-k}} + e^{-ik\pi/2d}\ketbra{x_{d-k}}{x_k}\right)\\
%	B_2 = \ketbra{x_0}{x_0} + 
%	\sum_{k=1}^{(d-1)/2}\left( e^{ik\pi/2d}\ketbra{x_{d-k}}{x_k} + e^{-ik\pi/2d}\ketbra{x_{k}}{x_{d-k}}\right)
%\end{align}
\begin{align}
	B_1 &= \ketbra{x_0}{x_0} + 
	\sum_{k=1}^{(d-1)/2}\left( e^{ik\pi/d}\ketbra{x_k}{x_{d-k}} + e^{-ik\pi/d}\ketbra{x_{d-k}}{x_k}\right)\\
	B_2 &= \ketbra{x_0}{x_0} + 
	\sum_{k=1}^{(d-1)/2}\left(\ketbra{x_{d-k}}{x_k} + \ketbra{x_k}{x_{d-k}}\right).
\end{align}
It can be checked that $B_1B_2 = \sum_{i=0}^{d-1} \omega_d^i \ketbra{x_i}{x_i}$ and the operator $U$ is
\begin{align}
	U = \sum_{i=0}^{d-1} \ketbra{x_{i/2 \pmod{d}}}{x_i}.
\end{align}
The first step of the embedding procedure in the proof of \cref{prop:embed} requires
the commutativity between $U$ and $B_2$, which can be verified as follows,
\begin{align}
	UB_2U\ct = U\ketbra{x_0}{x_0}U\ct + \sum_{i=1}^{d-1} U\ketbra{x_i}{x_{d-i}}U\ct 
	= \ketbra{x_0}{x_0} + \sum_{i=1}^{d-1} U\ketbra{x_i}{x_{d-i}}U\ct.
\end{align}
There are two cases to consider which are the index is even or odd.
Note that in the following calculation, the division is not with respect to the modulo $d$.
When the index $i$ is even, $U\ketbra{x_i}{x_{d-i}}U = \ketbra{x_{i/2}}{x_{d-i/2}}$, and
when the index $i$ is odd, $U\ketbra{x_i}{x_{d-i}}U = \ketbra{x_{(d+i)/2}}{x_{(d-i)/2}}$,
so we can calculate that 
\begin{align}
	\sum_{i=1}^{d-1} U\ketbra{x_i}{x_{d-i}}U\ct 
	=& \sum_{i=1}^{(d-1)/2}U\ketbra{x_{2i}}{x_{d-2i}}U\ct 
	+ \sum_{i=0}^{(d-1)/2-1} U\ketbra{x_{2i+1}}{x_{d-(2i+1)}}U\ct\\
	=& \sum_{i=1}^{(d-1)/2}\ketbra{x_i}{x_{d-i}} + \sum_{i=0}^{(d-1)/2-1} \ketbra{x_{(d+1)/2+i}}{x_{(d-1)/2-i}}\\
	=& \sum_{i=1}^{(d-1)/2}\ketbra{x_i}{x_{d-i}} + \sum_{i=(d+1)/2}^{d-1} \ketbra{x_i}{x_{d-i}}\\
	=&\sum_{i=1}^{d-1} \ketbra{x_i}{x_{d-i}}.
\end{align}
The decomposition of $U$ into $B_3$ and $B_4$ is similar to the decomposition of $X$ and we leave it to the curious 
readers.

In the next step of embedding, for each $x_i$, we find $y_i$ ,$z_i$, $w_i$ and $f$ such that
$x_i = y_iz_i = fw_i$. We demonstrate how this splitting is done by pick $x_1$ as an example, 
which is mapped to $B_1$ in the previous step. Now we map $x_1$ to $B_1'$ which is
\begin{align}
	B_1' = \begin{pmatrix}
	B_1 & 0 \\
	0 & B_1
	\end{pmatrix},
\end{align}
then $y_1$, $z_1$, $w_1$ and $f$ are mapped to
\begin{align}
y_1 \to 
\begin{pmatrix}
B_1 & 0\\
0 & \1
\end{pmatrix},
&&
z_1 \to
\begin{pmatrix}
\1 & 0\\
0 & B_1
\end{pmatrix},
\\
w_1 \to 
\begin{pmatrix}
0 & B_1\\
B_1 & 0
\end{pmatrix},
&&
f \to
\begin{pmatrix}
0 & \1\\
\1 & 0
\end{pmatrix}.
\end{align}

The last step of embedding follows the same manner as we double the dimension of the operators again.
For example, $y_1$ is mapped to $B_1 \oplus \1 \oplus B_1 \oplus \1$ and $z_1$ is mapped to 
$\1 \oplus B_1 \oplus \1 \oplus B_1$, so effectively, $x_1$ is mapped to $B_1^{\oplus 4}$.
\footnote{Details of this step of embedding can be found in proof of Proposition~$4.2$ in Ref.~\cite{slofstra2017}.}

In the end, we determine the shared state.
The optimal correlation for $\CHSH_X$ uses state 
$\ket{\psi} = \frac{1}{\sqrt{d-1}} \sum_{i=1}^{d-1} \ket{u_i}\ket{u_i}$, 
so the shared state for $\G$ is $\frac{1}{2}( \ket{\psi}^{\oplus 4})$.
\end{proof}
Next we are going to prove that the strategy winning this game optimally can self-test $d-1$-dimensional EPR pair.

%-----------------------------------------------------------------
\subsection{Proof of \cref{thm:pr_2}}
%-----------------------------------------------------------------
\hl{This subsection needs to be formalized.}

Suppose Alice and Bob achieve the optimal correlation with the quantum strategy $(\ket{\psi}, \{A_x\}, \{B_y\})$
for all $x,y \in \calX \times \calY$ .
The observables $A_x$ and $B_y$ and the shared state $\ket{\psi}$ shall not be confused with the ones used in the optimal strategy.
By Lemma~$4.3$ of Ref.~\cite{coladan2017}, we can extract an operator solution from the perfect winning strategy 
of the linear system game $\LS$. 
For each variable $\{ x_i \}_{i=1}^n$, Alice and Bob has operators $A_i$ and $B_i$ respectively.
The condition that they agree with assignment to variables means that 
\begin{align}
	\bra{\psi} A_i \otimes \overline{B_i} \ket{\psi} = 1 \Rightarrow A_i \otimes \overline{B_i} \ket{\psi} = \ket{\psi}
	\text{ for } 1 \leq i \leq n
\end{align}
and the condition that Alice's assignments satisfy the constraint means that 
\begin{align}
	\Tr(\rho_A \Pi_{j: H_{ij} \neq 0} A_j) = \Tr(\rho_A) \text{ for all } 1 \leq i \leq m
\end{align}
where $\rho_A =  \Tr_B(\ketbra{\psi}{\psi})$. 
Similarly we define $\rho_B = \Tr_A(\ketbra{\psi}{\psi})$.
For any $\ket{v} \in \supp(\rho_A)$,
we have 
\begin{align}
\Pi_{j:H_{ij} \neq 0} A_j \ket{v} = \ket{v} \text{ for all } 1 \leq i \leq m.
\end{align}
Since the relation $uxu^{-1} = x^2$ is embedded in this linear system game, we know
\begin{align}
	A_3A_4 A_1A_2 (A_3A_4)^\dagger \ket{v}= (A_1A_2)^2 \ket{v} \text{ for all } \ket{v} \in \supp(\rho_A).
\end{align}
How $u$ and $x$ are splitted into order-$2$ generators can be found in Appendix~\ref{sec:construct}.
For simplicity, we define $X_A = A_1A_2$ and $U_A=A_3A_4$ such that
the condition is equivalent to
\begin{align}
	\label{eq:ux_relation}
	U_AX_AU_A^\dagger \ket{v} = X_A^2 \ket{v} \text{ for all } \ket{v} \in \supp(\rho_A).
\end{align}
We will come back to the implication of this condition later.

Suppose $A_{m+1} = A_\ast^\diamond- A_\ast^\perp = \Pi_{V_A} - \Pi_{V_A^\perp}$m, where $V_A$ is a
$2m$-dimensional vector space and $\Pi_{V_A}$ is the projector onto it. The reason why it has dimension $2m$
will be clear shortly. On Bob's side, we also have $B_{n+1} = B_\ast^\diamond- B_\ast^\perp = \Pi_{V_B} - \Pi_{V_B^\perp}$.
Recall that from the extended weighted CHSH test, we know
\begin{align}
	A_\ast^\diamond \ket{\psi} = B_\ast^\diamond \ket{\psi} = A_\ast^\diamond \x B_\ast^\diamond \ket{\psi},
\end{align}
which implies that with an appropriate change of basis, we can get $V_A = V_B$, so in the rest of the proof
we drop the subscript of $V$.
By \cref{lm:chsh_comp} and \cref{prop:2d-subspace}, we know $V$ consists of $\omega_d$-eigenvectors and $\omega_d^{-1}$ eigenvectors of 
$X_A$, so we can write 
\begin{align}
	\Pi_{V} = \Pi_{V_1} + \Pi_{V_{d-1}},
\end{align}
where $\Pi_{V_{1}}$ is the projector onto the $\omega_d$-eigenspace of $X_A$ and $\Pi_{V_{d-1}}$ is the projector 
onto the $\omega_d^{-1}$-eigenspace of $X_A$.
Suppose $\ket{x_{1}} \in V_{1}$, then $X_A \ket{x_{1}} = \omega_d \ket{x_{1}}$.
Hence, by \cref{eq:ux_relation} we can calculate the eigen-decomposition of $X_A$ in $\supp(\rho_A)$.
as follows
\begin{align}
\label{eq:ladder}
 X_AU_A^\dagger \ket{x_{1}} = U_A^\dagger X_A^2 \ket{x_{1}} = \omega_d^2 U_A^\dagger \ket{x_{1}},
\end{align}
so $U_A^\dagger \ket{x_{1}}$ is an eigenvector of $X_A$ with eigenvalue $\omega_d^2$.
By induction, we know $X_A (U_A^\dagger)^i \ket{x_{1}} = \omega_d^{2^i} (U_A^\dagger)^i\ket{x_{A,1}}$. 
From the set $\{(U_A^\dagger)^i \Pi_{V_1} (U_A)^i \}_{i=0}^{d-2}$, we can identify $\Pi_{V_i}$ for $i = 1 \dots  d-1$
such that $\Pi_{V_i}$ is the projector onto the $\omega_d^i$-eigenspace of $X_A$,
and 
\begin{align}
 \cup_{i \in [d-1]+1} V_i \subset \supp(\rho_A).
\end{align}
Since unitary transformation does not change the rank of a matrix, we know $\rank(\Pi_{V_1}) = \rank(\Pi_{V_{i}}) =m$
for $ i =1 \dots d-1$,
and we assume 
\begin{align}
 \Pi_{V_i} = \sum_{j=1}^m \ketbra{x_{i,j}}{x_{i,j}}.
\end{align}
By \cref{eq:ux_relation} we also know that $U_A \ket{x_{i,j}} = \ket{x_{i/2,j}}$ for $i = 1,2 \dots d-1$.
In order to apply \cref{lm:ux_independ}, we construct $m$ subspaces $\{W_j\}_{j=1}^m$ where 
\begin{align}
	W_j = \spn( \{ \ket{x_{i,j}} \}_{i=1}^{d-1} )
\end{align}
The subspace $W_j$ is orthogonal to $W_{j'}$ for $j \neq j'$, and
$U_A$ and $X_A$ satisfy the condition of \cref{lm:ux_independ} when their actions are 
restricted to each $W_j$.
Similar argument also applies to operator $X_B$ and $U_B$ on Bob's side.

%Similar argument applies to Bob's operators. We can identify $X_B = B_1B_2$, 
%$U_B=B_3B_4$ and a vector space $S_B =\spn\{ \ket{x_{B,1}}, \ket{x_{B,2}} \dots  \ket{x_{B,d-1}} \} \subset \supp(\rho_B)$,
%such that 
%\begin{align}
%	X_B = \sum_{i=1}^{d-1} \omega_d^i \ketbra{x_{B,i}}{x_{B,i}} && U_B = \sum_{i=1}^{d-1} \ketbra{x_{B,i/2}}{x_{B,i}},
%\end{align}
%when their actions are restricted to $S_B$. Moreover, we can define a unitary $V: \supp{\rho_B} \to \supp(\rho_A)$,
%which maps $\ket{x_{B,i}}$ to $\ket{x_{A,i}}$ for $i=1\dots d-1$.


%and projectors $\Pi_{S_A}$ and $\Pi_{S_B}$ to be the projectors onto $S_A$ and $S_B$ respectively.
%From now on, we focus on the state $\ket{\psi_S}$ which is defined as
%\begin{align}
%	\ket{\psi_S} = (\Pi_{S_A} \x (V\Pi_{S_B}))\ket{\psi} = \vc(\tau) 
%\end{align}
%for some $\tau \in L(S_A)$.
With an appropriate change of basis, we 
assume that $\ket{\psi} = \vc(\tau)$ for some $\tau \in L(\supp(\rho_A))$
The consistency condition is equivalent to
\begin{align}
\label{eq:con_tau}
	A_i \tau B_i^\dagger = \tau.
\end{align}
Substituting $i=1,2$ into \cref{eq:con_tau}, we get 
\begin{align}
	X_A \tau X_A^\dagger = A_1A_2\tau B_2^\dagger B_1^\dagger = A_1\tau B_1^\dagger = \tau.
\end{align}
Similar argument gives us that 
\begin{align}
	U_A \tau U_A^\dagger = \tau.
\end{align}
Then we can conclude that for any $k \in \{0,1 \dots d-2\}$ and $l \in \{1,2\dots d-1\}$
\begin{align}
	U_A^kX_A^l \tau (U_A^kX_A^l)^\dagger = \tau.
\end{align}
Let $\Pi_{W_j}$ be the projector onto $W_j$. 
By \cref{lm:ux_independ}, $\Pi_{W_j} \tau \Pi_{W_j}$ commutes with all the $(d-1) \times (d-1)$ matrices, which means that 
\begin{align}
	\label{eq:d-1}
	\Pi_{W_j} \tau \Pi_{W_j}  = c_j \1_{S_A} \text{ for } j = 1\dots m.
\end{align}
In the vector form, we know
\begin{align}
	\sum_{j =1}^m \Pi_{W_j} \ket{\psi} =\sum_{i=1}^{d-1} \sum_{j=1}^m c_j \ket{x_{i,j}}\ket{x_{i,j}}. 
\end{align}
Recalling the fact that 
\begin{align}
\norm{ \Pi_{V_1} + \Pi_{V_{d-1}} \ket{\psi}}^2 = \norm{\Pi_{V_1} \ket{\psi}}^2 + \norm{\Pi_{V_{d-1}} \ket{\psi}}^2 = \frac{2}{d-1},
 \end{align}
 it means that 
 \begin{align}
 	2 \sum_{j=1}^m \norm{c_j}^2 = \frac{2}{d-1}.
 \end{align}
 Then we can calculate the norm of $\sum_{j =1}^m \Pi_{W_j} \ket{\psi}$, which is
 \begin{align}
 \norm{\sum_{j =1}^m \Pi_{W_j} \ket{\psi}}^2 = \sum_{i=1}^{d-1} \sum_{j=1}^m \norm{c_j}^2 = 1 = \norm{\ket{\psi}}^2.
 \end{align}
 In the end, we can conclude that $\supp{\rho_A} = \cup_{i=1}^{d-1} V_i$ and there exist local isometries $\Phi_A$ and $\Phi_B$
 such that 
 \begin{align}
 	\Phi_A\x\Phi_B  \ket{\psi} =  (\frac{1}{\sqrt{d-1}} \sum_{i=1}^{d-1} \ket{ii}) \x \ket{junk}.
 \end{align}
 
%The effect of $\CHSH_X$ and $\SVT_X$ is that we have a set $\{ \ket{x_i} \}_{i \in [d]} \in \supp(\rho_A)$ such that
%\begin{align}
%	X \ket{x_i} = \omega_d^i \ket{x_i}.
%\end{align}
%The other effect of $\SVT_X$ is that we know 
%\begin{align}
%	\tA_\triangle^\diamond \rho_A \tA_\triangle^\diamond = \frac{1}{d} \ketbra{x_0}{x_0} = \frac{1}{d} \tA_\triangle^\diamond.
%\end{align}
%\hl{\textbf{Question}: Can we show $\Tr(\ketbra{x_2}{x_2} \rho_A) = 1/d$?}
%========================================
\section{Extending $\G$ for general prime numbers }
\label{sec:extend}
%========================================



\bibliographystyle{alphaurl}
\bibliography{quantum_correlation}
\appendix
%========================================
\section{Proof of \cref{thm:selftest} }
\label{sec:selftest}
%========================================
\begin{proof}
Following the techniques developed in Ref.~\cite{bamps2015}, the first step is to find a sum-of-square decomposition of 
\begin{align}
	\bar{\I}_\alpha = 2\sqrt{\alpha^2+1} \1 - \I_\alpha
	= \frac{2}{\sin(\mu)} \1 - \frac{\cos(\mu)}{\sin(\mu)}(A_0B_0+A_0B_1) -  A_1B_0 + A_1B_1.
\end{align} 
With the following notation
\begin{align*}
	Z_A = A_0 &\quad X_A = A_1\\
	Z_B = \frac{B_0+B_1}{2\cos(\mu)} &\quad X_B = \frac{B_0-B_1}{2\sin(\mu)},
\end{align*}
the two SOS decompositions that we use are
\begin{align}
	\label{eq:sos1}&\bar{\I}_\alpha = \frac{\sin(\mu)\bar{\I}_\alpha^2 + 4\sin(\mu)\cos(\mu)^2(Z_AX_B+X_AZ_B)^2}{4},\\
	\label{eq:sos2}&\bar{\I}_\alpha = \frac{\cos^2(\mu)}{\sin(\mu)}(Z_A-Z_B)^2 + \sin(\mu) (X_A-X_B)^2.
\end{align}
The verification is omitted here.

Suppose the quantum strategy $(\ket{\psi}, \{\tilde{A}_x\}_{x \in [2]}, \{\tilde{B}_{y \in [2]}\}$ achieves that 
$\bra{\psi} \bar{\I}_\alpha \ket{\psi} \leq \epsilon$.
The second step is to establish bounds of the following form
\begin{align}
	&\|(\tilde{Z}_A-\tilde{Z}_B)\ket{\psi}\| \leq c_1 \sqrt{\epsilon}\\
	&\|(\tilde{X}_A(\1+\tilde{Z}_B)-\tilde{X}_B(\1-\tilde{Z}_A))\ket{\psi}\| \leq c_2 \sqrt{\epsilon}\\
	&\|(\tilde{X}_A-\tilde{X}_B)\ket{\psi}\| \leq c_3 \sqrt{\epsilon}\\
	&\|(\tilde{Z}_A\tilde{X}_A+\tilde{X}_A\tilde{Z}_A)\ket{\psi}\| \leq c_4 \sqrt{\epsilon}.
\end{align}
Now we write $s = \sin(\mu)$, $c = \cos(\mu)$ and define
\begin{align*}
	S_1 &= \frac{\sqrt{s}}{2} \bar{\I}_\alpha, \quad
	S_2 = \sqrt{s}c(\tilde{Z}_A\tilde{X}_B+\tilde{X}_A\tilde{Z}_B),\\
	S_3 &= \frac{c}{\sqrt{s}}(\tilde{Z}_A-\tilde{Z}_B),\quad
	S_4 = \sqrt{s}(\tilde{X}_A-\tilde{X}_B)
\end{align*}
then $\bar{\I}_\alpha = S_1^2 + S_2^2 = S_3^2 + S_4^2$ and $\bra{\psi}\bar{\I}_\alpha \ket{\psi} \leq \epsilon$ implies that 
$\bra{\psi}S^2_i \ket{\psi} \leq \epsilon$ and $\|S_i \ket{\psi} \| \leq \sqrt{\epsilon}$ for $i = 1,2,3,4$.
We can easily check that 
\begin{align*}
	c_1 = \frac{\sqrt{s}}{c}, \quad
	c_2 = \frac{1}{\sqrt{s}} + \frac{1}{c\sqrt{s}}, \quad
	c_3 = \frac{1}{\sqrt{s}}.
\end{align*}
where we use the relation that $\tilde{X}_A(\1+\tilde{Z}_B)-\tilde{X}_B(\1-\tilde{Z}_A) = S_4/s^{1/2} + S_2/(cs^{1/2})$.
To calculate $c_4$, we use the relation
\begin{align}
	\tilde{Z}_A\tilde{X}_A + \tilde{X}_A\tilde{Z}_A = \frac{S_2}{c\sqrt{s}} + \frac{\sqrt{s}\tilde{X}_AS_3}{c} + \frac{\tilde{Z}_AS_4}{\sqrt{s}}
\end{align}
and reach the conclusion that  
\begin{align}
	c_4 = \frac{1+c+s}{c\sqrt{s}}
\end{align}
where we use that fact that $\tilde{Z}_A, \tilde{X}_A$ are unitaries.

With appropriate substitutions, the rest of the proof follows the same derivation as that in Appendix A of Ref.~\cite{bamps2015},
so we omit it here.
\end{proof}
%%=====================================
\section{Equations to embed $uxu^{-1} = x^2$}
\label{sec:power2}
%%=====================================
\input{power2_equations}

%%=====================================
\section{Equations to embed $uxu^{-1} = x^3$}
\label{sec:power3}
%%=====================================
\input{power3_equations}

%%=====================================
\section{Equations to embed $uxu^{-1} = x^5$}
\label{sec:power5}
%%=====================================
\input{power5_equations}

\end{document}
